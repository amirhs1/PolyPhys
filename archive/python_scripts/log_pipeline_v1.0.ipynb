{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I gonna test the *neighbor*, *neigh_modify*, and *processors* commands of *Lammps*. To this end, I choose the details of these commands in the following way:\n",
    "1. processors Px Py Pz:\n",
    "\n",
    "    1.1. Px, Py, and Pz are the # of processors in direction x, y, and z, respectively.\n",
    "    \n",
    "    1.2. For 2 and 4 cores, I use _processors 1 1 *_.\n",
    "    \n",
    "    1.3. For 8, 16, 32 cores, I use _processors 2 2 *_.\n",
    "2. neigbor rskin bin:\n",
    "\n",
    "    2.1 rskin is the extra distance beyond the rcutoff of the potential. I use WCA (purely repulsive Lennard-Jones potential) with $r_{cutoff}=2^{1/6}\\sigma$ where the size (diameter) of an LJ bead $\\sigma=a_m=1.0$ and $a_m$ is the monomers size size, so $r_{skin}=rskin*\\sigma$.\n",
    "    \n",
    "    2.2 In my test, $rskin=0.2,0.3,0.4,$ and $0.5$.\n",
    " \n",
    "3. neigh_modify delay every check page one:\n",
    "\n",
    "    3.1 *delay* can be 0 or a multiple of every. Here, $delay=0,1,2,4,10,20$.\n",
    "    \n",
    "    3.2 *every* is set to $1,2,$ or $4$.\n",
    "    \n",
    "    3.3 *page* is set to $3000$, or $100000$.\n",
    "    \n",
    "    3.4 check is always *yes*.\n",
    "    \n",
    "    3.5. *one* is set to $300$, or $2000$.\n",
    "\n",
    "4. I also test *recenter* off and on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_CLXSgvVO_9"
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "# Importing necessary packages:\n",
    "import sys\n",
    "import os\n",
    "#import scipy.integrate as integrate\n",
    "#import scipy.special as special\n",
    "#from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import math\n",
    "import re\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from PipeLine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dirs = glob('../N*-logs/')\n",
    "path_to_save  = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_outputs(logs_path, path_to_save='./'):\n",
    "    outname = logs_path.split(\"/\")[1]\n",
    "    outdir = os.path.dirname(os.path.abspath(path_to_save))\n",
    "    dirpath = os.path.join(outdir,\"log_summary\")\n",
    "    try:\n",
    "        os.mkdir(dirpath)\n",
    "    except (FileExistsError, OSError):\n",
    "        print (\"The directory %s exits or is failed to create.\" % dirpath)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % dirpath)\n",
    "    # if dangerous build needed \n",
    "    details_out = dirpath + \"/\" + outname + \"_details.csv\"\n",
    "    with open(details_out, \"w\") as detailsfile:\n",
    "        # neigh_modify delay NUM every NUM check YES/NO:\n",
    "        detailsfile.write('groupname,filename,ens,run_seg,rskin,delay,every,check,epsilon,dcrowd,ncrowd,lcyl,dcyl,nmon,total_time_s,cores,timestep,atoms,ts_per_sec,')\n",
    "        # Section columns: min time, avg time, max time, %varavg, %total\"\n",
    "        # Section rows: Pair, Bond, Neigh, Comm, Output, Modify, Other\n",
    "        detailsfile.write('pair_avg_s,pair_pct,bond_avg_s,bond_pct,neigh_avg_s,neigh_pct,comm_avg_s,comm_pct,output_avg_s,output_pct,modify_avg_s,modify_pct,other_avg_s,other_pct,dangerous\\n')\n",
    "    \n",
    "    \n",
    "    runtime_out = dirpath + \"/\" + outname + \"_runtime.csv\"\n",
    "    with open(runtime_out, \"w\") as runfile:\n",
    "        runfile.write('groupname,filename,ncores,natoms,wall_time\\n')\n",
    "        runfile.close()\n",
    "    return details_out , runtime_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are to files.\n",
    "def lammps_log_details(logs_path, details_out , runtime_out):\n",
    "    log_files = glob(logs_path)\n",
    "    for file in log_files:\n",
    "        filename = file.split('.log')\n",
    "        filename = filename[0]\n",
    "        filename = filename.split('/')[-1]\n",
    "        ens = filename.split('ens')[-1]\n",
    "        groupname = filename.split('ens')[0]\n",
    "        with open(file,'r') as log,\\\n",
    "        open(details_out, \"a\") as detailsfile,\\\n",
    "        open(runtime_out, \"a\") as runfile:   \n",
    "            line = log.readline()\n",
    "            # The other of while loop are important\n",
    "            #neigh_modify delay NUM every NUM check YES/NO page NUM one NUM: \n",
    "            j = 1\n",
    "            while line:\n",
    "\n",
    "                while line.startswith('variable'):\n",
    "                    words = line.split()\n",
    "                    line = log.readline()\n",
    "                    if words[1] == 'epsilon1':\n",
    "                        epsilon = words[3]\n",
    "\n",
    "                    if words[1] == 'sig2':\n",
    "                        dcrowd = words[3]  \n",
    "\n",
    "                    if words[1] == 'n_crowd':\n",
    "                        ncrowd = words[3] \n",
    "\n",
    "                    if words[1] == 'lz':\n",
    "                        lcyl = str(2*float(words[3]))  \n",
    "\n",
    "                    if words[1] == 'r':\n",
    "                        dcyl = str(2*float(words[3]))\n",
    "\n",
    "                    if words[1] == 'n_bug':\n",
    "                        nmon = words[3]\n",
    "\n",
    "\n",
    "                if line.startswith('neighbor'):\n",
    "                    words = line.split()\n",
    "                    rskin = words[1].strip() # rskin\n",
    "\n",
    "                #neigh_modify delay NUM every NUM check YES/NO page NUM one NUM:  \n",
    "                if line.startswith('neigh_modify'):\n",
    "                    words = line.split()\n",
    "                    # picking the NUMs and Yes/No from neigh_modify command\n",
    "                    delay = words[2].strip()\n",
    "                    every = words[4].strip()\n",
    "                    check = words[6].strip()\n",
    "\n",
    "\n",
    "                if line.startswith('Loop time'):\n",
    "                    detailsfile.write(groupname)\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(filename)\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(ens)\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(str(j))#total time\n",
    "                    detailsfile.write(\",\")\n",
    "                    j += 1\n",
    "\n",
    "                    # neighbor and neigh_modify ocurres  one time but other occures 15 times.\n",
    "                    detailsfile.write(rskin) # rskin\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(delay) # delay\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(every) # every\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(check) # check\n",
    "                    detailsfile.write(\",\")\n",
    "\n",
    "                    detailsfile.write(epsilon) # epsilon\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(dcrowd) # dcrowd\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(ncrowd) # ncrowd\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(lcyl) # lcyl\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(dcyl) # dcyl\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(nmon) # nmon\n",
    "                    detailsfile.write(\",\")\n",
    "\n",
    "\n",
    "                    words = line.split()\n",
    "                    detailsfile.write(words[3].strip())#total time\n",
    "                    detailsfile.write(\",\")\n",
    "                    ncores = words[5].strip()\n",
    "                    detailsfile.write(ncores)# # of cores\n",
    "                    detailsfile.write(\",\")\n",
    "                    detailsfile.write(words[8].strip())# total timesteps\n",
    "                    detailsfile.write(\",\")\n",
    "                    natoms = words[11].strip()\n",
    "                    detailsfile.write(natoms)# total atoms\n",
    "                    detailsfile.write(\",\")                          \n",
    "\n",
    "                if line.startswith('Performance:'):\n",
    "                    words = line.split()\n",
    "                    detailsfile.write(words[3].strip())# timesteps per second\n",
    "                    detailsfile.write(\",\")\n",
    "\n",
    "                if line.startswith('Section'):\n",
    "                    _ = log.readline()\n",
    "                    for i in range(6): # Section rows: Pair, Bond, Neigh, Comm, Output, Modify, Other\n",
    "                        # Section columns: min time, avg time, max time, %varavg, %total\"\n",
    "                        line = log.readline()\n",
    "                        sect_min = line.split('|')[2].strip()\n",
    "                        detailsfile.write(sect_min)\n",
    "                        detailsfile.write(\",\")\n",
    "\n",
    "                        sect_pct = line.split()[-1] # Pair pct of total time\n",
    "                        detailsfile.write(sect_pct)\n",
    "                        detailsfile.write(\",\")\n",
    "\n",
    "                    line = log.readline()\n",
    "                    sect_min = line.split('|')[2].strip()\n",
    "                    detailsfile.write(sect_min)\n",
    "                    detailsfile.write(\",\")\n",
    "                    sect_pct = line.split()[-1] # Pair pct of total time\n",
    "                    detailsfile.write(sect_pct)\n",
    "                    detailsfile.write(\",\")\n",
    "\n",
    "                if line.startswith('Dangerous'):\n",
    "                    words = line.split()\n",
    "                    detailsfile.write(str(int(words[-1]))) # # number of dangerous builds\n",
    "                    #detailsfile.write(\",\")\n",
    "                    detailsfile.write(\"\\n\")\n",
    "\n",
    "                # runtime files\n",
    "                if line.startswith('Total wall time'):\n",
    "                    runfile.write(groupname)\n",
    "                    runfile.write(\",\")\n",
    "                    runfile.write(filename)\n",
    "                    runfile.write(\",\")\n",
    "                    runfile.write(ncores)\n",
    "                    runfile.write(\",\")\n",
    "                    runfile.write(natoms)\n",
    "                    runfile.write(\",\")\n",
    "                    words = line.split()\n",
    "                    runfile.write(words[-1]) # total wall time\n",
    "                    runfile.write(\"\\n\")\n",
    "\n",
    "                line = log.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_files = []\n",
    "runtime_files = []\n",
    "for logs_dir in logs_dirs:\n",
    "    logs_path = logs_dir+'/*.log'\n",
    "    details_out , runtime_out = log_outputs(logs_path, path_to_save='./')\n",
    "    details_files.append(details_out)\n",
    "    runtime_files.append(runtime_out)\n",
    "    lammps_log_details(logs_path, details_out , runtime_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = details_files + runtime_files\n",
    "log_file_pairs = PipeLine.file_reader(all_files,extensions=['_runtime.csv','_details.csv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_data = pd.read_csv(log_file_pairs[1][1])\n",
    "neigh_data.wall_time = pd.to_timedelta(neigh_data.wall_time)\n",
    "neigh_data['sum_pct']=neigh_data['neigh_pct']+neigh_data['comm_pct']\n",
    "neigh_data['sum_ave_s']=neigh_data['neigh_avg_s']+neigh_data['comm_avg_s']\n",
    "cols_sort = ['cores','sum_pct','neigh_pct','comm_pct','ts_per_sec','atoms']\n",
    "cols_ascending = [True,True,True,True,False,False]\n",
    "neigh_data.sort_values(cols_sort,inplace=True,ascending=cols_ascending)\n",
    "neigh_data.reset_index(inplace=True,drop=True)\n",
    "cols = ['cores','sum_pct','neigh_pct','comm_pct','ts_per_sec','total_time_s','sum_ave_s','dangerous']\n",
    "danger_runs = neigh_data[neigh_data.dangerous != 0]\n",
    "normal_runs = neigh_data[neigh_data.dangerous == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_data = pd.read_csv(log_file_pairs[1][0])\n",
    "runtime_data.wall_time = pd.to_timedelta(runtime_data.wall_time)\n",
    "runtime_data['total_time_hr'] = runtime_data.wall_time.dt.total_seconds() / 3600\n",
    "#groupname = [filename.split(\"ens\")[0] for filename in runtime_data.filename]\n",
    "#groupname =list(dict.fromkeys(groupname))\n",
    "cols = ['ncores','natoms','groupname']\n",
    "runtime_data.groupby(cols)['total_time_hr'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = runtime_data\n",
    "#facet_grid = sns.relplot(x=\"natoms\", y=\"total_time_hr\", hue='ncores', data=data)\n",
    "faced_grid = sns.relplot(x=\"natoms\", y=\"total_time_hr\", hue='ncores', kind=\"line\", ci=\"sd\", data=data);\n",
    "facet_grid.tight_layout()\n",
    "facet_grid.savefig(\"facet_plot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total run time for each set of simulations with 8 member.\n",
    "#unique_atoms = neigh_data.atoms.drop_duplicates().values\n",
    "unique_atoms = normal_runs.atoms.drop_duplicates().values\n",
    "runtime_dict = {'groupname':[],'natoms':[],'ncores':[],'nens':[],'sampling_counts':[],'sampling_mean_time_h':[],'total_counts':[],'total_mean_time_h':[]}\n",
    "#,'ncrowd':[],'nmon':[],'dcyl':[],'lcyl':[]}\n",
    "for atoms in unique_atoms:\n",
    "    #data = neigh_data.loc[(neigh_data.atoms == atoms) & (neigh_data.run_seg > 1)]\n",
    "    cond_atoms = normal_runs.atoms == atoms\n",
    "    cond_sampling = normal_runs.run_seg > 1\n",
    "    data = normal_runs.loc[(cond_atoms) & (normal_runs.run_seg > 1)] # seg_run =1 is for equilibration phase\n",
    "    if len(data.index) != 0:    \n",
    "        natoms = atoms\n",
    "        ncores = data.cores.drop_duplicates().values[0]\n",
    "        #filename = data.filename.drop_duplicates().values[0]\n",
    "        \n",
    "        # finding number of ensembles\n",
    "        cond_run_seg = normal_runs.run_seg == 2\n",
    "        nens = normal_runs[(cond_run_seg) & (cond_atoms)]['atoms'].value_counts().values[0]\n",
    "    \n",
    "        runtime_count = data.groupby('ens')['total_time_s'].count().sum() # shoud be nens*15\n",
    "        runtime_avg_h = data.groupby('ens')['total_time_s'].sum().mean()/3600\n",
    "        runtime_dict['natoms'].append(natoms)\n",
    "        #runtime_dict['filename'].append(filename)\n",
    "        runtime_dict['ncores'].append(ncores)\n",
    "        runtime_dict['nens'].append(nens)\n",
    "        runtime_dict['sampling_counts'].append(runtime_count)\n",
    "        runtime_dict['sampling_mean_time_h'].append(runtime_avg_h)\n",
    "    #data = neigh_data.loc[(neigh_data.atoms == atoms)]\n",
    "    \n",
    "    data = normal_runs.loc[(normal_runs.atoms == atoms)]\n",
    "    natoms = atoms\n",
    "    ncores = data.cores.drop_duplicates().values[0]\n",
    "    # finding number of ensembles\n",
    "    groupname = data.groupname.drop_duplicates().values[0] \n",
    "    runtime_dict['groupname'].append(groupname)\n",
    "    cond_run_seg = normal_runs.run_seg == 2\n",
    "    runtime_count = data.groupby('ens')['total_time_s'].count().sum() # shoud be nens*11=88\n",
    "    runtime_avg_h = data.groupby('ens')['total_time_s'].sum().mean()/3600\n",
    "    runtime_dict['total_counts'].append(runtime_count)\n",
    "    runtime_dict['total_mean_time_h'].append(runtime_avg_h)\n",
    "\n",
    "    \n",
    "runtime_df = pd.DataFrame.from_dict(runtime_dict)\n",
    "runtime_df['equilibration_mena_time_h'] = runtime_df['total_mean_time_h'] - runtime_df['sampling_mean_time_h']\n",
    "runtime_df['equilibration_counts'] = runtime_df['total_counts'] - runtime_df['sampling_counts']\n",
    "runtime_df.sort_values('total_mean_time_h',inplace=True)\n",
    "runtime_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total run time\n",
    "# \n",
    "data = runtime_df\n",
    "#filename = \"core2\"\n",
    "axis = sns.relplot(x=\"natoms\", y=\"total_mean_time_h\", hue='ncores', data=data)\n",
    "axis.tight_layout()\n",
    "axis.savefig(\"total_time.pdf\")\n",
    "#g = sns.FacetGrid(attend, col=\"subject\", col_wrap=4, height=2, ylim=(0, 10))\n",
    "#g.map(sns.pointplot, \"solutions\", \"score\", order=[1, 2, 3], color=\".3\", ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data = default_cpu2\n",
    "filename = \"core2\"\n",
    "facet_grid = sns.relplot(x=\"run_seg\", y=\"ts_per_sec\", hue='atoms',col=\"ens\",col_wrap=4, data=data)\n",
    "facet_grid.tight_layout()\n",
    "facet_grid.savefig(\"facet_plot\"+filename+\".pdf\")\n",
    "#g = sns.FacetGrid(attend, col=\"subject\", col_wrap=4, height=2, ylim=(0, 10))\n",
    "#g.map(sns.pointplot, \"solutions\", \"score\", order=[1, 2, 3], color=\".3\", ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate run time for new simulations.\n",
    "unique_atoms = neigh_data.atoms.drop_duplicates().values\n",
    "runtime_dict = {'natoms':[],'ncores':[],'nens':[],'runtime_count_sampling':[],'runtime_avg_sampling_h':[],'runtime_count':[],'runtime_avg_h':[]}\n",
    "for atoms in unique_atoms:\n",
    "    data = neigh_data.loc[(neigh_data.atoms == atoms) & (neigh_data.run_seg > 1)]\n",
    "    natoms = atoms\n",
    "    ncores = data.cores.drop_duplicates().values[0]\n",
    "    nens = 8\n",
    "    runtime_count = data.groupby('ens')['total_time_s'].count().sum() # shoud be 8*10=80\n",
    "    runtime_avg_h = data.groupby('ens')['total_time_s'].sum().mean()/3600\n",
    "    runtime_dict['natoms'].append(natoms)\n",
    "    runtime_dict['ncores'].append(ncores)\n",
    "    runtime_dict['nens'].append(nens)\n",
    "    runtime_dict['runtime_count_sampling'].append(runtime_count)\n",
    "    runtime_dict['runtime_avg_sampling_h'].append(runtime_avg_h)\n",
    "    \n",
    "    data = neigh_data.loc[(neigh_data.atoms == atoms)]\n",
    "    natoms = atoms\n",
    "    ncores = data.cores.drop_duplicates().values[0]\n",
    "    nens = 8\n",
    "    runtime_count = data.groupby('ens')['total_time_s'].count().sum() # shoud be 8*11=88\n",
    "    runtime_avg_h = data.groupby('ens')['total_time_s'].sum().mean()/3600\n",
    "    runtime_dict['runtime_count'].append(runtime_count)\n",
    "    runtime_dict['runtime_avg_h'].append(runtime_avg_h)\n",
    "runtime_df = pd.DataFrame.from_dict(runtime_dict)\n",
    "runtime_df['runtime_avg_equilibration_h'] = runtime_df['runtime_avg_h'] - runtime_df['runtime_avg_sampling_h']\n",
    "jnum = int(7e7/5e6) # total n# of timestep / # of timestep in one loop\n",
    "print(jnum)\n",
    "runtime_df['runtime_avg_sampling_10times_h'] = runtime_df['runtime_avg_sampling_h']*jnum/(runtime_df['runtime_count_sampling']/8)\n",
    "runtime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing log files based on the number of cores\n",
    "condition2 = (neigh_data.run_seg == 11)\n",
    "#condition2 = \n",
    "default_cpu2 = neigh_data.loc[(neigh_data.cores == 2)]\n",
    "default_cpu4 = neigh_data.loc[(neigh_data.cores == 4)]\n",
    "default_cpu8 = neigh_data.loc[(neigh_data.cores == 8)]\n",
    "#default_cpu16 = default_page.loc[default_page.cores == 16]\n",
    "#default_cpu32 = default_page.loc[default_page.cores == 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = default_cpu2\n",
    "facet_grid = sns.relplot(x=\"run_seg\", y=\"ts_per_sec\", hue='atoms',col=\"ens\",col_wrap=4, data=data)\n",
    "facet_grid.tight_layout()\n",
    "facet_grid.savefig(\"facet_plot.pdf\")\n",
    "#g = sns.FacetGrid(attend, col=\"subject\", col_wrap=4, height=2, ylim=(0, 10))\n",
    "#g.map(sns.pointplot, \"solutions\", \"score\", order=[1, 2, 3], color=\".3\", ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['cores','total_time_s','sum_pct','neigh_pct','comm_pct','ts_per_sec','atoms','dangerous','est_ttotal_s','est_ttotal_h']\n",
    "cols_sort =  ['ts_per_sec','sum_pct','neigh_pct','comm_pct','sum_ave_s']\n",
    "cols_ascending = [False,True,True,True,False]\n",
    "default_cpu2.reset_index(inplace=True,drop=True)\n",
    "default_cpu2.sort_values(cols_sort,ascending=cols_ascending)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column with the highest timestep_per_second on a 2 core machine\n",
    "default_cpu2.iloc[default_cpu2.ts_per_sec.idxmax()]\n",
    "# The difference in neigh_pct of this column with the column with lowest neigh_pct is less than 5% \n",
    "# however its comm_pct is half of that with the lowest neigh_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above analyze is for a system with 100080 atoms run for 100000 timesteps with timestep/sec=47.309\n",
    "# what about a system with 10000 atoms that run for 7*10^7 timesteps?\n",
    "# To answer this question we assume all the algorithm are of order O(N) where N is the number of atoms.\n",
    "data = default_cpu2\n",
    "col = 'ts_per_sec'\n",
    "idx = data[col].idxmax()\n",
    "idx = 7\n",
    "col = 'ts_per_sec'\n",
    "tp_per_sec_mean = default_cpu2[col].min()\n",
    "print(tp_per_sec_mean)\n",
    "test_natoms = data.loc[idx,'atoms']\n",
    "test_nsteps = data.loc[idx,'timestep']\n",
    "test_ttotal = data.loc[idx,'total_time_s']\n",
    "test_cores = data.loc[idx,'cores']\n",
    "#print(test_nsteps/test_ttotal)\n",
    "natoms = 2e3\n",
    "nsteps = 5e7\n",
    "ttotal = (nsteps/tp_per_sec_mean) # seconds\n",
    "ttotal_hr = ttotal/3600 # hours \n",
    "print(\"The estimated simulation time for a system with {} atoms in {} timesteps on {}-core machine is {} seconds or {} hours.\".format(natoms,nsteps,test_cores,ttotal,ttotal_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cpu4.reset_index(inplace=True,drop=True)\n",
    "default_cpu4.sort_values(cols_sort,ascending=cols_ascending)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cpu2.est_ttotal_h.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column with the highest timestep_per_second on a 4-core machine\n",
    "default_cpu4.iloc[default_cpu4.ts_per_sec.idxmax()]\n",
    "# The difference in neigh_pct of this column with the column with lowest neigh_pct is less than 5% \n",
    "# however its comm_pct is half of that with the lowest neigh_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cpu4.iloc[default_cpu4[col].idxmin()]['atoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above analyze is for a system with 100080 atoms run for 100000 timesteps with timestep/sec=47.309\n",
    "# what about a system with 10000 atoms that run for 7*10^7 timesteps?\n",
    "# To answer this question we assume all the algorithm are of order O(N) where N is the number of atoms.\n",
    "data = default_cpu4\n",
    "col = 'ts_per_sec'\n",
    "idx = data[col].idxmax()\n",
    "#idx = 7\n",
    "col = 'ts_per_sec'\n",
    "tp_per_sec_mean = default_cpu4[col].min()\n",
    "test_atoms = default_cpu4.iloc[default_cpu4[col].idxmin()]['atom']\n",
    "print(tp_per_sec_mean)\n",
    "print(test_atoms)\n",
    "test_natoms = data.loc[idx,'atoms']\n",
    "test_nsteps = data.loc[idx,'timestep']\n",
    "test_ttotal = data.loc[idx,'total_time_s']\n",
    "test_cores = data.loc[idx,'cores']\n",
    "#print(test_nsteps/test_ttotal)\n",
    "natoms = 45000\n",
    "nsteps = 5e7\n",
    "ttotal = (45000/)(nsteps/tp_per_sec_mean) # seconds\n",
    "ttotal_hr = ttotal/3600 # hours \n",
    "print(\"The estimated simulation time for a system with {} atoms in {} timesteps on {}-core machine is {} seconds or {} hours.\".format(natoms,nsteps,test_cores,ttotal,ttotal_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cpu8.reset_index(inplace=True,drop=True)\n",
    "default_cpu8.sort_values(cols_sort,ascending=cols_ascending)[cols][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The column with the highest timestep_per_second on a 2 core machine\n",
    "default_cpu8.iloc[default_cpu8.ts_per_sec.idxmax()]\n",
    "# The difference in neigh_pct of this column with the column with lowest neigh_pct is less than 5% \n",
    "# however its comm_pct is half of that with the lowest neigh_pct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above analyze is for a system with 100080 atoms run for 100000 timesteps with timestep/sec=47.309\n",
    "# what about a system with 10000 atoms that run for 7*10^7 timesteps?\n",
    "# To answer this question we assume all the algorithm are of order O(N) where N is the number of atoms.\n",
    "data = default_cpu8\n",
    "col = 'ts_per_sec'\n",
    "idx = data[col].idxmax()\n",
    "#idx = 7\n",
    "col = 'ts_per_sec'\n",
    "tp_per_sec_mean = default_cpu8[col].min()\n",
    "print(tp_per_sec_mean)\n",
    "test_natoms = data.loc[idx,'atoms']\n",
    "test_nsteps = data.loc[idx,'timestep']\n",
    "test_ttotal = data.loc[idx,'total_time_s']\n",
    "test_cores = data.loc[idx,'cores']\n",
    "#print(test_nsteps/test_ttotal)\n",
    "natoms = 45000\n",
    "nsteps = 5e7\n",
    "ttotal = (nsteps/tp_per_sec_mean) # seconds\n",
    "ttotal_hr = ttotal/3600 # hours \n",
    "print(\"The estimated simulation time for a system with {} atoms in {} timesteps on {}-core machine is {} seconds or {} hours.\".format(natoms,nsteps,test_cores,ttotal,ttotal_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cpu16.reset_index(inplace=True,drop=True)\n",
    "default_cpu16.sort_values(cols_sort,ascending=cols_ascending)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1='neigh_avg(s)'\n",
    "col2='comm_avg(s)'\n",
    "col3='sum_ave(s)'\n",
    "col4='ts_per_sec'\n",
    "default_cpu2_bar = default_cpu2[['shortname',col1,col2,col3,col4]]\n",
    "default_cpu4_bar = default_cpu4[['shortname',col1,col2,col3,col4]]\n",
    "default_cpu8_bar = default_cpu8[['shortname',col1,col2,col3,col4]]\n",
    "default_cpu16_bar = default_cpu16[['shortname',col1,col2,col3,col4]]\n",
    "default_cpu32_bar = default_cpu32[['shortname',col1,col2,col3,col4]]\n",
    "default_page_bar = default_page[['shortname',col1,col2,col3,col4]]\n",
    "default_cpu2_bar.sort_values([col3,col2,col1],inplace=True,ascending=False)\n",
    "default_cpu4_bar.sort_values([col3,col2,col1],inplace=True,ascending=False)\n",
    "default_cpu8_bar.sort_values([col3,col2,col1],inplace=True,ascending=False)\n",
    "default_cpu16_bar.sort_values([col3,col2,col1],inplace=True,ascending=False)\n",
    "default_cpu32_bar.sort_values([col3,col2,col1],inplace=True,ascending=False)\n",
    "default_page_bar.sort_values([col3,col2,col1],inplace=True,ascending=False)\n",
    "\n",
    "default_cpu2_bar = default_cpu2_bar[['shortname',col1,col2]]\n",
    "default_cpu4_bar = default_cpu4_bar[['shortname',col1,col2]]\n",
    "default_cpu8_bar = default_cpu8_bar[['shortname',col1,col2]]\n",
    "default_cpu16_bar = default_cpu16_bar[['shortname',col1,col2]]\n",
    "default_cpu32_bar = default_cpu32_bar[['shortname',col1,col2]]\n",
    "default_page_bar = default_page_bar[['shortname',col1,col2]]\n",
    "\n",
    "default_cpu2_bar.set_index('shortname',drop=True,inplace=True)\n",
    "default_cpu4_bar.set_index('shortname',drop=True,inplace=True)\n",
    "default_cpu8_bar.set_index('shortname',drop=True,inplace=True)\n",
    "default_cpu16_bar.set_index('shortname',drop=True,inplace=True)\n",
    "default_cpu32_bar.set_index('shortname',drop=True,inplace=True)\n",
    "default_page_bar.set_index('shortname',drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cpu2[['filename','ts_per_sec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,9))\n",
    "ax = default_cpu2_bar.plot.barh(stacked=True,figsize=(20, 30))\n",
    "ax.text(1700,30,\"For rskin=0.2, # of timesteps = 10000\")\n",
    "name = 'avg_min'\n",
    "ax.set_xlabel('Time (sec)')\n",
    "plt.savefig('neighbor_cpu2_'+name+'.pdf',dpi=300,format='pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
