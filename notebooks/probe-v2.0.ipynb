{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The probe phase\n",
    "There are several ways of analyzing the topology and trajectory pairs, depending on the number of trajectory files per a topology file, the continuity of trjaectory files, organization of files in a directory, and the parallel or sequencial arrangement of the computation powerhorse.\n",
    "\n",
    "\n",
    "### To-do-list\n",
    "\n",
    "This notebook is a living documnent. It is an integration of all the past notebooks and scripts written for the *probe* phase in the *PolyPhys* package (or formerly-called *extraction* phase in the decryped *sumrule* package)\n",
    "\n",
    "BThe list below allows to review the past notebooks and scripts and combine them into this notebook:\n",
    "\n",
    "- [] Sum-rule: segments: bug test and run on **date:NODATE-YET**\n",
    "- [] Sum-rule: segments: all test and run on **date:NODATE-YET**\n",
    "- [] Sum-rule: wholes: bug test and run on **date:NODATE-YET**\n",
    "- [] Sum-rule: wholes: all test and run on **date:NODATE-YET**\n",
    "- [] Sum-rule: all_in_one: test and run on **date:NODATE-YET**\n",
    "- [X] Trans-Foci: segments: bug test and run on **date:20220621**\n",
    "- [] Trans-Foci: segments: all test and run on **date:NODATE-YET**\n",
    "- [] Trans-Foci: wholes: bug test and run on **date:NODATE-YET**\n",
    "- [] Trans-Foci: wholes: all test and run on **date:NODATE-YET**\n",
    "- [] Trans-Foci: all_in_one: test and run on **date:NODATE-YET**\n",
    "\n",
    "### Naming convention:\n",
    "\n",
    "This is the pattern of file or directory names:\n",
    "\n",
    "1. **whole** files: whole-group-property_[-measure][-stage][.ext]\n",
    "2. **ensemble** files: ensemble-group-property_[-measure][-stage][.ext]\n",
    "3. **ensemble_long** files: ensemble_long-group-property_[-measure][-stage][.ext]\n",
    "4. **space** files: space-group-property_[-measure][-stage][.ext]\n",
    "5. **all in one** files: space-group-**species**-**allInOne**-property-_[-measure][-stage][.ext]\n",
    "\n",
    "[keyword] means that the keyword in the file name is option. [-measure] is a physical measurement such as the auto correlation function (AFC) done on the physical 'property_'.\n",
    "\n",
    "### Settings for testing and running on a PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_CLXSgvVO_9"
   },
   "outputs": [],
   "source": [
    "# settings for testing and running on a PC.\n",
    "from glob import glob\n",
    "import pathlib\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.manage.parser import SumRule, TransFoci\n",
    "from polyphys.probe import prober"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "client = Client(n_workers=2)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HPC Cluster: gnuparallel\n",
    "\n",
    "## Separated *whole* simulation directories\n",
    "\n",
    "On a cluster, *whole* simulations are organized into *whole* directories, where each *whole* directory contains all the files for a given *whole* simulation. The **gnuparallel** is used to parallalize the **probe** phase at the **shell** level. For this purpose, all the python modules and scripts are separatedly installed and run on each core. For instance, if 32 cores are available, then the files in 32 *whole* directories are simulatenously installed. However, each *whole* directory may contains multiple toplogy and trajectory pairs. Thus, there is parallelization at the level of *whole* directories, not at the levle of the *segment* or *whole* trajectories inside a *whole* directory. Inside each *whole* directory, a python **main_probe.py** script analyzes the trajectories in a sequencial way.\n",
    "\n",
    "- trj and all *segments* on a cluster\n",
    "\n",
    "For each *whole* directory, the following script is executed by means of *gnuparallel*. See these scripts: *probe-1.7-all_trj_segments.py* and *probe-1.7-bug_trj_segments*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-rule project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "\n",
    "\n",
    "# analyzing bug files.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "bug_pairs = glob('./N*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.sum_rule_bug(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        geometry,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### segment bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import SumRule\n",
    "\n",
    "\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "bug_trjs = glob('./N*' + group + '*')\n",
    "bug_trjs = organizer.sort_filenames(\n",
    "    bug_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "bug_trjs = [bug_trj[0] for bug_trj in bug_trjs]\n",
    "bug_topo = glob('./N*' + group + '*')\n",
    "bug_topo = organizer.sort_filenames(bug_topo, fmts=['.' + group + '.data'])\n",
    "bug_topo = bug_topo[0][0]\n",
    "max_segment_id = len(bug_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for bug_trj in bug_trjs[:1]:\n",
    "    trj_info = SumRule(\n",
    "        bug_trj,\n",
    "        geometry=geometry,\n",
    "        group=group,\n",
    "        lineage=topo_lineage\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_bug(\n",
    "            bug_topo, bug_trj, geometry, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.sum_rule_bug(\n",
    "            bug_topo,\n",
    "            bug_trj,\n",
    "            geometry,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import SumRule\n",
    "\n",
    "\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob('./N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob('./N*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs[:1]:\n",
    "    trj_info = SumRule(\n",
    "        all_trj,\n",
    "        geometry=geometry,\n",
    "        group=group,\n",
    "        lineage=topo_lineage\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_all(\n",
    "            all_topo, all_trj, geometry, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.sum_rule_all(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            geometry,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trans-Foci project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whole bug trjs\n",
    "Each *bug topology* comes with only **one** *bug trajectory*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "\n",
    "# analyzing bug files.\n",
    "geometry = 'biaxial'\n",
    "group='bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "bug_pairs = glob('./eps*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.trans_fuci_bug(\n",
    "        bug_topo, bug_trj, lineage, group, geometry, save_to=save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob('./eps*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob('./eps*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = TransFoci(\n",
    "        all_trj,\n",
    "        geometry=geometry,\n",
    "        group=group,\n",
    "        lineage=topo_lineage\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all(\n",
    "            all_topo, all_trj, geometry, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        prober.trans_foci_all(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            geometry,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC Serial scheme \n",
    "\n",
    "There are 4 different types of directories from which only one type can be in a *space* directory.\n",
    "\n",
    "There are separated **whole** directories in each of which there **all** and **bug** **whole** trajectories; or, there are again separated **whole** directories in each of which there are **all** and **bug** **segment** trjaectories. Below there are two groups of scrips for **serial** and **parallel** runnung schemes.\n",
    "\n",
    "\n",
    "On a PC, the *whole* directories are located in a master *space-trjs* directory; however, one main python script probes all the *whole* directories in a parallel scheme via Dask. This is different from the *gnuparallel*-based approach in which each *whole* directory has its own copy of the required scripts and a main pytohn script is run to probe that direcotry individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumrule project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the name of analyze file...\n",
      "\n",
      "N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7-bug is analyzing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirhsi/OneDrive - University of Waterloo/PhD Research/Jupyter/PolyPhys/polyphys/manage/parser.py:422: UserWarning: It is assumed that 'nc' is the last attribute shortkey in a lineage_name of types: 'ensemble', 'ensemble_long', 'whole', 'segment'.\n",
      "  warnings.warn(convention_warning, UserWarning)\n",
      "/Users/amirhsi/opt/miniconda3/envs/polyLab/lib/python3.9/site-packages/MDAnalysis/core/groups.py:440: DeprecationWarning: The 'pbc' kwarg has been deprecated and will be removed in version 3.0., please use 'wrap' instead\n",
      "  warnings.warn(\"The 'pbc' kwarg has been deprecated and will be \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m bug_pairs \u001b[38;5;241m=\u001b[39m organizer\u001b[38;5;241m.\u001b[39msort_filenames(\n\u001b[1;32m     10\u001b[0m     bug_pairs,\n\u001b[1;32m     11\u001b[0m     fmts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m group \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m group \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.lammpstrj\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (bug_topo, bug_trj) \u001b[38;5;129;01min\u001b[39;00m bug_pairs:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mprober\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_rule_bug\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbug_topo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbug_trj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_to\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/OneDrive - University of Waterloo/PhD Research/Jupyter/PolyPhys/polyphys/probe/prober.py:855\u001b[0m, in \u001b[0;36msum_rule_bug\u001b[0;34m(topology, trajectory, geometry, lineage, save_to, continuous)\u001b[0m\n\u001b[1;32m    853\u001b[0m asphericity_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    854\u001b[0m shape_parameter_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 855\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m sliced_trj:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# various measures of chain size\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     fsd_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(fsd_t, np\u001b[38;5;241m.\u001b[39marray([fsd(bug\u001b[38;5;241m.\u001b[39mpositions)]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    858\u001b[0m     gyr_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(gyr_t, np\u001b[38;5;241m.\u001b[39marray([bug\u001b[38;5;241m.\u001b[39mradius_of_gyration()]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/polyLab/lib/python3.9/site-packages/MDAnalysis/coordinates/base.py:1505\u001b[0m, in \u001b[0;36mProtoReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;124;03m\"\"\"Forward one step to next frame when using the `next` builtin.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/polyLab/lib/python3.9/site-packages/MDAnalysis/coordinates/base.py:1491\u001b[0m, in \u001b[0;36mProtoReader.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;124;03m\"\"\"Forward one step to next frame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1491\u001b[0m     ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_next_timestep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mEOFError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewind()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/polyLab/lib/python3.9/site-packages/MDAnalysis/coordinates/LAMMPS.py:629\u001b[0m, in \u001b[0;36mDumpReader._read_next_timestep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attr_to_col_ix\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_atoms):\n\u001b[0;32m--> 629\u001b[0m     fields \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids:\n\u001b[1;32m    631\u001b[0m         indices[i] \u001b[38;5;241m=\u001b[39m fields[attr_to_col_ix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# analyzing bug files.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "#macmini_path = \"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "macbookpro_path = \"/Users/amirhsi/Downloads/N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7\"\n",
    "bug_pairs = glob(macbookpro_path + '/N*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.sum_rule_bug(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        geometry,\n",
    "        lineage,\n",
    "        continuous=False,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### segment bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 min and 45 s for ~30000 particles with one all trj\n",
    "macbookpro_path = \"/Users/amirhsi/Downloads/N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7\"\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "bug_trjs = glob(macbookpro_path + '/N*' + group + '*')\n",
    "bug_trjs = organizer.sort_filenames(\n",
    "    bug_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "bug_trjs = [bug_trj[0] for bug_trj in bug_trjs]\n",
    "bug_topo = glob(macbookpro_path + '/N*' + group + '*')\n",
    "bug_topo = organizer.sort_filenames(bug_topo, fmts=['.' + group + '.data'])\n",
    "bug_topo = bug_topo[0][0]\n",
    "max_segment_id = len(bug_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for bug_trj in bug_trjs[:1]:\n",
    "    trj_info = SumRule(\n",
    "        bug_trj,\n",
    "        geometry=geometry,\n",
    "        group=group,\n",
    "        lineage=topo_lineage\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_bug(\n",
    "            bug_topo, bug_trj, geometry, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.sum_rule_bug(\n",
    "            bug_topo,\n",
    "            bug_trj,\n",
    "            geometry,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4 mins for ~4000 particles with one all trj\n",
    "macbookpro_path = \"/Users/amirhsi/Downloads/N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7\"\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob(macbookpro_path + '/N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "print(all_trjs)\n",
    "all_topo = glob(macbookpro_path + '/N*' + group + '*')\n",
    "print(all_topo)\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs[:1]:\n",
    "    trj_info = SumRule(\n",
    "        all_trj,\n",
    "        geometry=geometry,\n",
    "        group=group,\n",
    "        lineage=topo_lineage\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_all(\n",
    "            all_topo, all_trj, geometry, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.sum_rule_all(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            geometry,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trans-Foci project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing bug files.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "linage = 'whole'\n",
    "save_to = './'\n",
    "#macmini_path = \"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "macbookpro_path = \"/Users/amirhsi/Downloads/epss5epsl5r10.5al5nl5ml125ns400ac1nc27720lz77dt0.005bdump2000adump5000ens8\"\n",
    "bug_pairs = glob(macbookpro_path + '/eps*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.trans_fuci_bug(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        geometry,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 min and 45 s for ~30000 particles with one all trj\n",
    "#macbookpro_path = \"/Users/amirhsi/Downloads/epss5epsl5r10.5al5nl5ml125ns400ac1nc27720lz77dt0.005bdump2000adump5000ens8\"\n",
    "macmini_path = '/Users/amirhsi_mini/research_data/test'\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob(macmini_path + '/eps*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob(macmini_path + '/eps*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs[:1]:\n",
    "    trj_info = TransFoci(\n",
    "        all_trj,\n",
    "        geometry=geometry,\n",
    "        group=group,\n",
    "        lineage=topo_lineage\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all(\n",
    "            all_topo, all_trj, geometry, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        prober.trans_foci_all(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            geometry,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parallel scheme with dask\n",
    "\n",
    "There are 4 different types of directories from which only one type can be in a *space* directory.\n",
    "\n",
    "There are separated **whole** directories in each of which there **all** and **bug** **whole** trajectories; or, there are again separated **whole** directories in each of which there are **all** and **bug** **segment** trjaectories. Below there are two groups of scrips for **serial** and **parallel** runnung schemes.\n",
    "\n",
    "\n",
    "## *bug* *segment* trjs:\n",
    "\n",
    "On a PC, the *whole* directories are located in a master *space-trjs* directory; however, one main python script probes all the *whole* directories in a parallel scheme via Dask. This is different from the *gnuparallel*-based approach in which each *whole* directory has its own copy of the required scripts and a main pytohn script is run to probe that direcotry individually.\n",
    "\n",
    "## *bug* *whole* trjs: NOT tested 20220603: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This approach from HERE\n",
    "path = pathlib.Path('/Users/amirhsi_mini/trjs/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "## to HERE, does not work of * is used in the string input for Path.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.bug.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.bug.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = organizer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "trjs_computed = []\n",
    "for topology in topologies:\n",
    "    print(topology[0])\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='whole')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            trj_delayed = delayed(prober.sum_rule_bug)(topology[0], trajectory[0], geometry, 'whole', save_to_whole, continuous=False)\n",
    "            trjs_computed.append(trj_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# it takes 9min and 34s.\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *bug* *segment* trjs: NONT written: NOT tested 20220603: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *all* *segment* trjs: probe_all: NOT dask style: NOT tested 20220603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.all.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.all.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = organizer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "for topology in topologies:\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.segment_id ==10:\n",
    "            prober.sum_rule_all(topology[0], trajectory[0], geometry, 'segment', save_to_whole)\n",
    "        else:\n",
    "            prober.sum_rule_all(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "all_tuples =  organizer.sort_filenames(observations,fmts=['all.lammpstrj'])\n",
    "all_trjs = [all_tuple[0] for all_tuple in all_tuples]\n",
    "all_data =  organizer.sort_filenames(observations,fmts=['all.data'])\n",
    "all_data = all_data[0][0]\n",
    "\n",
    "    \n",
    "for all_trj in all_trjs:\n",
    "    print(all_trj)\n",
    "    #PipeLine.extract_trj_all(all_data, all_trj, geom, save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *all* *segment* trjs: probe_all_new:: NOT tested 20220603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('/Users/amirhsi_mini/trjs/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.all.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.all.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = organizer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "trjs_computed = []\n",
    "for topology in topologies:\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            if trj_info.segment_id ==14:\n",
    "                trj_delayed = delayed(prober.probe_all_new)(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=False)\n",
    "                trjs_computed.append(trj_delayed)\n",
    "            else:\n",
    "                trj_delayed = delayed(prober.probe_all_new)(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)\n",
    "                trjs_computed.append(trj_delayed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = compute(trjs_computed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "905eddd834f47278e82c8803001c8cba7ff23ca6cd99b3eaaa45ba7f5342ab1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
