{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The probe phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HPC Cluster: gnuparallel\n",
    "\n",
    "## Separated *whole* simulation directories\n",
    "\n",
    "On a cluster, *whole* simulations are organized into *whole* directories, where each *whole* directory contains all the files for a given *whole* simulation. The **gnuparallel** is used to parallelize the **probe** phase at the **shell** level. For this purpose, all the python modules and scripts are separatedly installed and run on each core. For instance, if 32 cores are available, then the files in 32 *whole* directories are simulatenously installed. However, each *whole* directory may contains multiple topology and trajectory pairs. Thus, there is parallelization at the level of *whole* directories, not at the level of the *segment* or *whole* trajectories inside a *whole* directory. Inside each *whole* directory, a python **main_probe.py** script analyzes the trajectories in a sequential way.\n",
    "\n",
    "- trj and all *segments* on a cluster\n",
    "\n",
    "For each *whole* directory, the following script is executed by means of *gnuparallel*. See these scripts: *probe-1.7-all_trj_segments.py* and *probe-1.7-bug_trj_segments*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sum-rule project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "\n",
    "# 24 minutes\n",
    "# analyzing bug files.\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "database = '/Users/amirhsi_mini/research_data/sumrule/'\n",
    "#database = './'\n",
    "hierarchy = 'N*'\n",
    "bug_pairs = glob(database + hierarchy + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "print(bug_pairs)\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    print(bug_topo)\n",
    "    prober.sum_rule_bug_cyl(bug_topo, bug_trj, lineage, save_to=save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### segment bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import SumRuleCyl\n",
    "\n",
    "\n",
    "group = 'bug'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "bug_trjs = glob('./N*' + group + '*')\n",
    "bug_trjs = organizer.sort_filenames(\n",
    "    bug_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "bug_trjs = [bug_trj[0] for bug_trj in bug_trjs]\n",
    "bug_topo = glob('./N*' + group + '*')\n",
    "bug_topo = organizer.sort_filenames(bug_topo, fmts=['.' + group + '.data'])\n",
    "bug_topo = bug_topo[0][0]\n",
    "max_segment_id = len(bug_trjs)\n",
    "# analyzing all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for bug_trj in bug_trjs:\n",
    "    trj_info = SumRuleCyl(\n",
    "        bug_trj, topo_lineage, 'cylindrical', group, 'linear'\n",
    "        )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_bug_cyl(bug_topo, bug_trj, lineage, save_to=save_to)\n",
    "    else:\n",
    "        prober.sum_rule_bug_cyl(\n",
    "            bug_topo,\n",
    "            bug_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import SumRuleCyl\n",
    "\n",
    "\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob('./N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob('./N*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzing all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = SumRuleCyl(\n",
    "        all_trj, topo_lineage, 'cylindrical', group, 'linear'\n",
    "        )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_all_cyl(all_topo, all_trj, lineage, save_to=save_to)\n",
    "    else:\n",
    "        prober.sum_rule_all_cyl(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trans-Foci project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cylindrical\n",
    "#### whole bug trjs\n",
    "Each *bug topology* comes with only **one** *bug trajectory*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import TransFociCyl\n",
    "\n",
    "# analyzing bug files.\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "bug_pairs = glob('./eps*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.trans_foci_bug_cyl(bug_topo, bug_trj, lineage, save_to=save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob('./eps*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob('./eps*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = TransFociCyl(all_trj, topo_lineage, 'cylindrical', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all_cyl(all_topo, all_trj, lineage, save_to=save_to)\n",
    "    else:\n",
    "        prober.trans_foci_all_cyl(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### whole bug trjs\n",
    "Each *bug topology* comes with only **one** *bug trajectory*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import TransFociCub\n",
    "\n",
    "# analyzing bug files.\n",
    "geometry = 'cubic'\n",
    "group='bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "bug_pairs = glob('/Users/amirhsi/research_data/TransFociCub-probe/al*/al*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    print(bug_topo)\n",
    "    print(bug_trj)\n",
    "    #prober.trans_foci_bug_cub(bug_topo, bug_trj, lineage, save_to=save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### segment bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import TransFociCub\n",
    "\n",
    "\n",
    "geometry = 'cubic'\n",
    "group = 'bug'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "#all_trjs = glob('./al*' + group + '*')\n",
    "all_dirs = glob('/Users/amirhsi/research_data/TransFociCub-probe/al*/')\n",
    "for all_dir in all_dirs:\n",
    "    all_trjs = glob(all_dir + 'al*' + group + '*')\n",
    "    all_trjs = organizer.sort_filenames(\n",
    "        all_trjs,\n",
    "        fmts=['.' + group + '.lammpstrj']\n",
    "    )\n",
    "    all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "    all_topo = glob(all_dir + 'al*' + group + '*')\n",
    "    all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "    all_topo = all_topo[0][0]\n",
    "    max_segment_id = len(all_trjs)\n",
    "    print(max_segment_id)\n",
    "    # analyzig all files\n",
    "    # it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "    for all_trj in all_trjs:\n",
    "        trj_info = TransFociCub(all_trj, topo_lineage, geometry, group, 'ring')\n",
    "        # all the frames in the last segment are probed:\n",
    "        if trj_info.segment_id == max_segment_id:\n",
    "            prober.trans_foci_bug_cub(\n",
    "                all_topo, all_trj, lineage, save_to=save_to\n",
    "            )\n",
    "        else:\n",
    "            prober.trans_foci_bug_cub(\n",
    "                all_topo,\n",
    "                all_trj,\n",
    "                lineage,\n",
    "                save_to=save_to,\n",
    "                continuous=True\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for all_trj in all_trjs:\n",
    "    print(all)\n",
    "    trj_info = TransFociCub(all_trj, topo_lineage, 'cubic', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all_cub(\n",
    "            all_topo, all_trj, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.trans_foci_all_cub(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob('./al*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob('./al*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = TransFociCub(all_trj, topo_lineage, 'cubic', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all_cub(\n",
    "            all_topo, all_trj, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.trans_foci_all_cub(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob('./al*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob('./al*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = TransFociCub(all_trj, topo_lineage, 'cubic', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all_cub(\n",
    "            all_topo, all_trj, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.trans_foci_all_cub(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PC Serial scheme\n",
    "\n",
    "There are 4 different types of directories from which only one type can be in a *space* directory.\n",
    "\n",
    "There are separated **whole** directories in each of which there **all** and **bug** **whole** trajectories; or, there are again separated **whole** directories in each of which there are **all** and **bug** **segment** trajectories. Below there are two groups of scrips for **serial** and **parallel** running schemes.\n",
    "\n",
    "\n",
    "On a PC, the *whole* directories are located in a master *space-trjs* directory; however, one main python script probes all the *whole* directories in a parallel scheme via Dask. This is different from the *gnuparallel*-based approach in which each *whole* directory has its own copy of the required scripts and a main python script is run to probe that directory individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwoMonDep project\n",
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirhsi/miniconda3/envs/polylab/lib/python3.11/site-packages/Bio/Application/__init__.py:40: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
      "\n",
      "Due to the on going maintenance burden of keeping command line application\n",
      "wrappers up to date, we have decided to deprecate and eventually remove these\n",
      "modules.\n",
      "\n",
      "We instead now recommend building your command line and invoking it directly\n",
      "with the subprocess module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from typing import Optional, Dict, Any, Union\n",
    "import numpy as np\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis import transformations as mda_trans\n",
    "from MDAnalysis.analysis import distances as mda_dist\n",
    "from polyphys.manage.parser import (\n",
    "    SumRuleCyl, TransFociCyl, TransFociCub, HnsCub, HnsCyl,\n",
    "    SumRuleCubHeteroLinear, SumRuleCubHeteroRing, TwoMonDep\n",
    "    )\n",
    "from polyphys.manage.typer import ParserT\n",
    "from polyphys.manage.organizer import invalid_keyword\n",
    "from polyphys.analyze import clusters, correlations\n",
    "from polyphys.analyze.measurer import transverse_size, fsd, end_to_end, pair_distance, apply_pbc_orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stamps_report_with_measures(\n",
    "    report_name: str,\n",
    "    sim_info: Union[TransFociCub, TransFociCyl],\n",
    "    n_frames: int,\n",
    "    measures: Dict[str, float],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes a summary of stamps (properties and attributes) of a simulation\n",
    "    to file.\n",
    "\n",
    "    `stamps_report` generates a dataset called `report_name` of the\n",
    "    values of some attributes of `sim_info`, the number of frames\n",
    "    `n_frames`, all the key and value pairs in all the given dictionaries\n",
    "    `measures`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    report_name: str\n",
    "        Name of the report.\n",
    "    sim_info: ParserT\n",
    "        A ParserT instant object that contains information about the name,\n",
    "        parents,and physical attributes of a simulation.\n",
    "    n_frames: int\n",
    "        Number of frames/snapshots/configurations in a simulation.\n",
    "    measures: Dict\n",
    "        A dictionary of measures where a key and value pair is the name and\n",
    "        value of a physical property.\n",
    "    \"\"\"\n",
    "    with open(report_name, mode='w') as report:\n",
    "        # write header\n",
    "        for lineage_name in sim_info._genealogy:\n",
    "            report.write(f\"{lineage_name},\")\n",
    "        for attr_name in sim_info.attributes:\n",
    "            report.write(f\"{attr_name},\")\n",
    "        for measure_name in measures.keys():  # each measure is a dict\n",
    "            report.write(f\"{measure_name},\")\n",
    "        report.write(\"n_frames\\n\")\n",
    "        # write values\n",
    "        for lineage_name in sim_info.genealogy:\n",
    "            attr_value = getattr(sim_info, lineage_name)\n",
    "            report.write(f\"{attr_value},\")\n",
    "        for attr_name in sim_info.attributes:\n",
    "            attr_value = getattr(sim_info, attr_name)\n",
    "            report.write(f\"{attr_value},\")\n",
    "        for measure_value in measures.values():\n",
    "            report.write(f\"{measure_value},\")\n",
    "        report.write(f\"{n_frames}\")\n",
    "\n",
    "\n",
    "def stamps_report(\n",
    "    report_name: str,\n",
    "    sim_info: ParserT,\n",
    "    n_frames: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes a summary of stamps (properties and attributes) of a simulation\n",
    "    to file.\n",
    "\n",
    "    `stamps_report` generates a dataset called `report_name` of the\n",
    "    values of some attributes of `sim_info`, the number of frames\n",
    "    `n_frames`, all the key and value pairs in all the given dictionaries\n",
    "    `measures`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    report_name: str\n",
    "        Name of the report.\n",
    "    sim_info: ParserT\n",
    "        A SumRule object that contains information about the name, parents,\n",
    "        and physical attributes of a simulation.\n",
    "    n_frames: int\n",
    "        Number of frames/snapshots/configurations in a simulation.\n",
    "    \"\"\"\n",
    "    with open(report_name, mode='w') as report:\n",
    "        # write header\n",
    "        for lineage_name in sim_info.genealogy:\n",
    "            report.write(f\"{lineage_name},\")\n",
    "        for attr_name in sim_info.attributes:\n",
    "            report.write(f\"{attr_name},\")\n",
    "        report.write(\"n_frames\\n\")\n",
    "        # write values\n",
    "        for lineage_name in sim_info.genealogy:\n",
    "            attr_value = getattr(sim_info, lineage_name)\n",
    "            report.write(f\"{attr_value},\")\n",
    "        for attr_name in sim_info.attributes:\n",
    "            attr_value = getattr(sim_info, attr_name)\n",
    "            report.write(f\"{attr_value},\")\n",
    "        report.write(f\"{n_frames}\")\n",
    "\n",
    "\n",
    "def bin_create(\n",
    "    sim_name: str,\n",
    "    edge_name: str,\n",
    "    bin_size: float,\n",
    "    lmin: float,\n",
    "    lmax: float,\n",
    "    save_to: str\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates arrays of bins and histograms\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sim_name: str\n",
    "        Name of the simulation.\n",
    "    edge_name: str\n",
    "        Name of the variable for which the histogram is computed.\n",
    "    bin_size : float\n",
    "        Size of each bin.\n",
    "    lmin : float\n",
    "        Lower bound of the system in the direction of interest.\n",
    "    lmax : float\n",
    "        Upper bound of the system in the direction of interest.\n",
    "    save_to : str\n",
    "        Whether save outputs to memory as csv files or not.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bin_edges : numpy array of float\n",
    "        The edges to pass into a histogram. Save `bin_edges` to file if\n",
    "        `save_to` is not None.\n",
    "    hist: array of int\n",
    "        An empty histogram\n",
    "    \"\"\"\n",
    "    bin_edges = np.arange(lmin, lmax + bin_size, bin_size)\n",
    "    hist = np.zeros(len(bin_edges) - 1, dtype=np.int16)\n",
    "    np.save(save_to + sim_name + '-' + edge_name + '.npy', bin_edges)\n",
    "    return bin_edges, hist\n",
    "\n",
    "\n",
    "def fixedsize_bins(\n",
    "    sim_name: str,\n",
    "    edge_name: str,\n",
    "    bin_size: float,\n",
    "    lmin: float,\n",
    "    lmax: float,\n",
    "    bin_type: str = 'ordinary',\n",
    "    save_to: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates arrays of bins and histograms, ensuring that the `bin_size`\n",
    "    guaranteed. To achieve this, it extends the `lmin` and `lmax` limits.\n",
    "\n",
    "    To-do List\n",
    "    ----------\n",
    "    1. Following the idea used:\n",
    "    https://docs.mdanalysis.org/1.1.1/_modules/MDAnalysis/lib/util.html#fixedwidth_bins\n",
    "    Makes input array-like so bins can be calculated for 1D data (then all\n",
    "    parameters are simple floats) or nD data (then parameters are supplied\n",
    "    as arrays, with each entry corresponding to one dimension).\n",
    "    2. Eliminate the if-statement for the periodic_bin_edges.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sim_name: str\n",
    "        Name of the simulation.\n",
    "    edge_name: str\n",
    "        Name of the variable for which the histogram is computed.\n",
    "    bin_size : float\n",
    "        Size of each bin.\n",
    "    lmin : float\n",
    "        Lower bound of the system in the direction of interest.\n",
    "    lmax : float\n",
    "        Upper bound of the system in the direction of interest.\n",
    "    bin_type: {'ordinary', 'nonnegative', 'periodic'}, default 'ordinary'\n",
    "        The type of bin in a given direction in a given coordinate system:\n",
    "\n",
    "        'ordinary'\n",
    "            A bounded or unbounded coordinate such as any of the cartesian\n",
    "            coordinates or the polar coordinate in the spherical coordinate\n",
    "            system. For such coordinates, the `lmin` and `lmax` limits are\n",
    "            equally extended to ensure the `bin_size`.\n",
    "\n",
    "        'nonnegative'\n",
    "            A nonnegative coordinate such as the r direction in the polar\n",
    "            or spherical coordinate system. For such coordinates, ONLY `lmax`\n",
    "            limit is extended to ensure the `bin_size`. `lmin` is either 0.0\n",
    "            or a positive number smaller than `lmax`.\n",
    "\n",
    "        'periodic'\n",
    "            A periodic coordinate such as the azimuthal direction in the\n",
    "            spherical coordinate. It is assumed that 'period'=`lmax`-`lmin`;\n",
    "            therefore, if 'period' is not a multiple of `bin_size`, then an\n",
    "            array of bin_edges is used; otherwise, n_bins is used.\n",
    "\n",
    "    save_to : str, default None\n",
    "        Whether save outputs to memory as npy files or not.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bin_edges : numpy array of  float\n",
    "        The edges to pass into a histogram. Save `bin_edges` to file if\n",
    "        `save_to` is not None.\n",
    "    hist: array of  int\n",
    "        An empty histogram\n",
    "\n",
    "    Reference:\n",
    "    https://docs.mdanalysis.org/1.1.1/documentation_pages/lib/util.html#MDAnalysis.analysis.density.fixedwidth_bins\n",
    "    \"\"\"\n",
    "    hist_collectors = 0\n",
    "    bin_edges = 0\n",
    "    bin_types = ['ordinary', 'nonnagative', 'periodic']\n",
    "    if lmin >= lmax:\n",
    "        raise ValueError('Boundaries are not sane: should be xmin < xmax.')\n",
    "    _delta = bin_size\n",
    "    _lmin = lmin\n",
    "    _lmax = lmax\n",
    "    _length = _lmax - _lmin\n",
    "    n_bins: int = 0\n",
    "    if bin_type == 'ordinary':\n",
    "        n_bins = int(np.ceil(_length / _delta))\n",
    "        dl = 0.5 * (n_bins * _delta - _length)  # excess length\n",
    "        # add half of the excess to each end:\n",
    "        _lmin = _lmin - dl\n",
    "        _lmax = _lmax + dl\n",
    "        # create empty grid with the right dimensions (and get the edges)\n",
    "        hist_collectors, bin_edges = np.histogram(\n",
    "            np.zeros(1),\n",
    "            bins=n_bins,\n",
    "            range=(_lmin, _lmax)\n",
    "        )\n",
    "    elif bin_type == 'nonnegative':\n",
    "        n_bins = int(np.ceil(_length / _delta))\n",
    "        dl = 0.5 * (n_bins * _delta - _length)\n",
    "        _lmin = _lmin - dl\n",
    "        _lmax = _lmax + dl\n",
    "        if _lmin <= 0.0:\n",
    "            _lmin = 0.0\n",
    "            # add full of the excess to upper end:\n",
    "            _lmax = _lmax + 2 * dl\n",
    "        hist_collectors, bin_edges = np.histogram(\n",
    "            np.zeros(1),\n",
    "            bins=n_bins,\n",
    "            range=(_lmin, _lmax)\n",
    "        )\n",
    "    elif bin_type == 'periodic':  # Assuming that the _length=period:\n",
    "        n_bins = int(np.ceil(_length / _delta))\n",
    "        warnings.warn(\n",
    "            f\"Number of bins (n_bins='{n_bins}')\"\n",
    "            \" is more than or equal to the actual number of bins in \"\n",
    "            f\"'periodic' bin type because the 'period=lmax-min={_length}'\"\n",
    "            f\"and delta='{_delta}'\"\n",
    "            \",not 'n_bins', are used to created 'bin_edges'.\",\n",
    "            UserWarning\n",
    "            )\n",
    "        bin_edges = np.arange(_lmin, _lmax + _delta, _delta)\n",
    "        hist_collectors, bin_edges = np.histogram(\n",
    "            np.zeros(1),\n",
    "            bins=bin_edges,\n",
    "            range=(_lmin, _lmax)\n",
    "        )\n",
    "    else:\n",
    "        invalid_keyword(bin_type, bin_types)\n",
    "    hist_collectors = hist_collectors * 0\n",
    "    hist_collectors_std = hist_collectors * 0\n",
    "    if save_to is not None:\n",
    "        np.save(save_to + sim_name + '-' + edge_name + '.npy', bin_edges)\n",
    "    results = {\n",
    "        'n_bins': n_bins,\n",
    "        'bin_edges': bin_edges,\n",
    "        'collector': hist_collectors,\n",
    "        'collector_std': hist_collectors_std,\n",
    "        'range': (_lmin, _lmax)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def write_hists(\n",
    "    hist_infos: Dict[str, Any],\n",
    "    sim_name: str,\n",
    "    save_to: str,\n",
    "    std: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes histogram per species per direction to file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist_infos : Dict[str, Any]\n",
    "        A dict of dicts that contains the information about direction, species,\n",
    "         and histograms.\n",
    "    sim_name: str\n",
    "        The name of simulation file to which the `hist_infos` belongs.\n",
    "    save_to: str\n",
    "        The absolute/relative path of a directory to which outputs are saved.\n",
    "    std : bool, default False\n",
    "        _description_, by default False\n",
    "    \"\"\"\n",
    "    for dir_ in hist_infos.keys():\n",
    "        for species, hist in hist_infos[dir_].items():\n",
    "            np.save(\n",
    "                save_to + sim_name + '-' + dir_ + species + '.npy',\n",
    "                hist['collector']\n",
    "            )\n",
    "            if std is True:\n",
    "                np.save(\n",
    "                    save_to + sim_name + '-' + dir_ + 'Std' + species + '.npy',\n",
    "                    hist['collector_std']\n",
    "                )\n",
    "        # end of loop\n",
    "    # end of loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_pair_distance(\n",
    "    positions: np.ndarray,\n",
    "    pbc: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the adjusted pair distance between two particles along each axis\n",
    "    in the Cartesian coordinate system, applying the minimum image convention\n",
    "    (MIC) and correcting for jumps across boundaries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    positions : numpy.ndarray\n",
    "        A (2, n_dim) array containing the coordinates of the two atoms.\n",
    "        The array should be sorted by atom number, with the first row \n",
    "        corresponding to the first atom and the second row to the second atom.\n",
    "\n",
    "    pbc : numpy.ndarray\n",
    "        An array where the index are the dimensions (0 for x, 1 for y, \n",
    "        and 2 for z) and the values are the lengths of the simulation box \n",
    "        in those dimensions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A 1D numpy array containing the adjusted pair distances along each axis.\n",
    "    \"\"\"\n",
    "    n_atoms, n_dims = positions.shape\n",
    "    pbc_inv = 1/ pbc\n",
    "    if n_atoms != 2:\n",
    "        raise ValueError(\"'adjusted_pair_distance' only works for two atoms.\")\n",
    "    \n",
    "    # Calculation in the center of geometry of the atom group\n",
    "    positions = positions - np.mean(positions, axis=0)\n",
    "    \n",
    "    # Compute the raw pair distances\n",
    "    dr = positions[1] - positions[0]\n",
    "    # Apply MIC and correct for jumps\n",
    "    dr_adj = np.where(np.abs(dr) <= pbc - np.abs(dr), dr, dr - pbc * np.around(pbc_inv * dr))\n",
    "    return dr_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([25, 25, 25])\n",
    "dr = np.array([13, 10, 24])\n",
    "\n",
    "np.where(dr < 0.5 * lengths, dr, np.sign(dr) * (lengths - abs(dr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sign(dr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_mon_dep_cub_bug(\n",
    "    topology: str,\n",
    "    trajectory: str,\n",
    "    lineage: str,\n",
    "    save_to: str = './',\n",
    "    continuous: bool = False\n",
    ") -> None:\n",
    "    \"\"\"Runs various analyses on a `lineage` simulation of a 'bug' atom group in\n",
    "    the `geometry` of interest.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    In this project, coordinates are wrapped and unscaled in a\n",
    "    trajectory or topology file; moreover, LAMMPS recenter is used to\n",
    "    restrict the center of mass of \"bug\" (monomers) to the center of\n",
    "    simulation box; and consequently, coordinates of all the particles in a\n",
    "    trajectory or topology file is recentered to fulfill this constraint.\n",
    "\n",
    "    In MDAnalysis, selections by `universe.select_atoms` always return an\n",
    "    AtomGroup with atoms sorted according to their index in the topology.\n",
    "    This feature is used below to measure the end-to-end distance (Flory\n",
    "    radius), genomic distance (index differnce along the backbone), and any\n",
    "    other measurement that needs the sorted indices of atoms,bonds, angles, and\n",
    "    any other attribute of an atom.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topology: str\n",
    "        Name of the topology file.\n",
    "    trajectory: str\n",
    "        Name of the trajectory file.\n",
    "    lineage: {'segment', 'whole'}\n",
    "        Type of the input file.\n",
    "    save_to: str, default './'\n",
    "        The absolute/relative path of a directory to which outputs are saved.\n",
    "    continuous: bool, default False\n",
    "        Whether a `trajectory` file is a part of a sequence of trajectory\n",
    "        segments or not.\n",
    "    \"\"\"\n",
    "    if (lineage == 'segment') & (continuous is False):\n",
    "        warnings.warn(\n",
    "            \"lineage is \"\n",
    "            f\"'{lineage}' \"\n",
    "            \"and 'continuous' is \"\n",
    "            f\"'{continuous}. \"\n",
    "            \"Please ensure the \"\n",
    "            f\"'{trajectory}' is NOT part of a sequence of trajectories.\",\n",
    "            UserWarning\n",
    "        )\n",
    "    print(\"Setting the name of analyze file...\")\n",
    "    sim_info = TwoMonDep(\n",
    "        trajectory,\n",
    "        lineage,\n",
    "        'cubic',\n",
    "        'bug',\n",
    "        'atom'\n",
    "    )\n",
    "    sim_name = sim_info.lineage_name + \"-\" + sim_info.group\n",
    "    print(\"\\n\" + sim_name + \" is analyzing...\\n\")\n",
    "    # LJ time difference between two consecutive frames:\n",
    "    time_unit = sim_info.dcrowd * np.sqrt(\n",
    "        sim_info.mcrowd * sim_info.eps_others)  # LJ time unit\n",
    "    lj_nstep = sim_info.bdump  # Sampling steps via dump command in Lammps\n",
    "    lj_dt = sim_info.dt\n",
    "    sim_real_dt = lj_nstep * lj_dt * time_unit\n",
    "    cell = mda.Universe(\n",
    "        topology, trajectory, topology_format='DATA',\n",
    "        format='LAMMPSDUMP', lammps_coordinate_convention='unscaled',\n",
    "        atom_style=\"id type x y z\", dt=sim_real_dt\n",
    "        )\n",
    "    #cell_pbc = {0: sim_info.lcube, 1: sim_info.lcube, 2: sim_info.lcube}\n",
    "    cell_pbc = np.array([sim_info.lcube, sim_info.lcube, sim_info.lcube])\n",
    "    # slicing trajectory based the continuous condition\n",
    "    if continuous:\n",
    "        sliced_trj = cell.trajectory[0: -1]\n",
    "        n_frames = cell.trajectory.n_frames - 1\n",
    "    else:\n",
    "        sliced_trj = cell.trajectory\n",
    "        n_frames = cell.trajectory.n_frames\n",
    "    # selecting atom groups\n",
    "    bug: mda.AtomGroup = cell.select_atoms('type 1')  # bug: small & large mon\n",
    "    print(len(bug.atoms))\n",
    "    # defining collectors\n",
    "    # -bug:\n",
    "    gyr_t = np.zeros(n_frames)\n",
    "    bug_x_t = np.zeros(n_frames)\n",
    "    bug_y_t = np.zeros(n_frames)\n",
    "    bug_z_t = np.zeros(n_frames)\n",
    "    for idx, _ in enumerate(sliced_trj):\n",
    "        # bug:\n",
    "        # various measures of chain size\n",
    "        gyr_t[idx] = bug.radius_of_gyration()\n",
    "        dr_ij = adjusted_pair_distance(bug.positions, cell_pbc)\n",
    "        bug_x_t[idx] = dr_ij[0]\n",
    "        bug_y_t[idx] = dr_ij[1] \n",
    "        bug_z_t[idx] = dr_ij[2]\n",
    "    # Saving collectors to memory\n",
    "    # -bug\n",
    "    np.save(save_to + sim_name + '-gyrTMon.npy', gyr_t)\n",
    "    np.save(save_to + sim_name + '-xTMon.npy', bug_x_t)\n",
    "    np.save(save_to + sim_name + '-yTMon.npy', bug_y_t)\n",
    "    np.save(save_to + sim_name + '-zTMon.npy', bug_z_t)\n",
    "    # Simulation stamps:\n",
    "    outfile = save_to + sim_name + \"-stamps.csv\"\n",
    "    stamps_report(outfile, sim_info, n_frames)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing bug files.\n",
    "\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "import os\n",
    "\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = '/Users/amirhsi/research_data/TwoMonDep-probes/'\n",
    "#path =\n",
    "#\"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "path = \"/Users/amirhsi/research_data/TwoMonDep-all_simulations/am*\"\n",
    "bug_pairs = glob(path + '/am*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "#print(bug_pairs)\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    parent = os.path.dirname(os.path.abspath(bug_topo))\n",
    "    # Get the name of the parent directory\n",
    "    parent = os.path.basename(parent)\n",
    "    print(parent)\n",
    "    two_mon_dep_cub_bug(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        lineage,\n",
    "        continuous=False,\n",
    "        save_to = save_to+parent+'/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sumrule project\n",
    "### Cylindrical\n",
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing bug files.\n",
    "\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "\n",
    "\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "#path = \"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "path = \"/Users/amirhsi/research_data/N*/N*\"\n",
    "bug_pairs = glob(path + '/N*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.sum_rule_bug_cyl(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        lineage,\n",
    "        continuous=False,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 min and 45 s for ~30000 particles with one all trj\n",
    "input_path = \"/Users/amirhsi/Downloads/N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7\"\n",
    "group = 'bug'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "bug_trjs = glob(input_path + '/N*' + group + '*')\n",
    "bug_trjs = organizer.sort_filenames(\n",
    "    bug_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "bug_trjs = [bug_trj[0] for bug_trj in bug_trjs]\n",
    "bug_topo = glob(input_path + '/N*' + group + '*')\n",
    "bug_topo = organizer.sort_filenames(bug_topo, fmts=['.' + group + '.data'])\n",
    "bug_topo = bug_topo[0][0]\n",
    "max_segment_id = len(bug_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for bug_trj in bug_trjs:\n",
    "    trj_info = SumRuleCyl(\n",
    "        bug_trj,\n",
    "        topo_lineage,\n",
    "        'cylindrical',\n",
    "        group,\n",
    "        'linear'\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_bug_cyl(\n",
    "            bug_topo, bug_trj, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.sum_rule_bug_cyl(\n",
    "            bug_topo,\n",
    "            bug_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trjs: hist_1d and hist_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 mins for ~4000 particles with one all trj\n",
    "parent = \"/Users/amirhsi_mini/research_data/trjs\"\n",
    "#parent = \"/Users/amirhsi/Downloads/N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7\"\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob(parent + '/N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "print(all_trjs)\n",
    "all_topo = glob(parent + '/N*' + group + '*')\n",
    "print(all_topo)\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = SumRuleCyl(\n",
    "        all_trj,\n",
    "        topo_lineage,\n",
    "        'cylindrical',\n",
    "        group,\n",
    "        'linear'\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_all_cyl(\n",
    "            all_topo, all_trj, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.sum_rule_all_cyl(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trjs: hist_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 10 mins for N2000epsilon5.0r13.0lz259.0sig1.0nc133547dt0.005bdump1000adump5000ens1 2 all trjs\n",
    "trjs_db = \"/Users/amirhsi_mini/research_data/trjs/N2000epsilon5.0r13.0lz259.0sig1.0nc133547dt0.005bdump1000adump5000ens1\"\n",
    "#trjs_db = \"/Users/amirhsi/Downloads/N2000epsilon5.0r15.5lz379.5sig6.0nc1779dt0.005bdump1000adump5000ens7\"\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob(trjs_db + '/N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "print(all_trjs)\n",
    "all_topo = glob(trjs_db + '/N*' + group + '*')\n",
    "print(all_topo)\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = SumRuleCyl(\n",
    "        all_trj,\n",
    "        topo_lineage,\n",
    "        'cylindrical',\n",
    "        group,\n",
    "        'linear'\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.sum_rule_all_cyl_hist2d(\n",
    "            all_topo, all_trj, lineage, save_to=save_to\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        prober.sum_rule_all_cyl_hist2d(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SumRuleCubHeteroLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing bug files.\\\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "#macmini_path = \"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "macbookpro_path = \"/Users/amirhsi/research_data/\"\n",
    "bug_pairs = glob(macbookpro_path + '/al*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "print(bug_pairs)\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.sum_rule_hetero_linear_bug_cub(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trans-Foci project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cylindrical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing bug files.\n",
    "group = 'bug'\n",
    "linage = 'whole'\n",
    "save_to = './'\n",
    "#macmini_path = \"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "macbookpro_path = \"/Users/amirhsi/Downloads/epss5epsl5r10.5al5nl5ml125ns400ac1nc27720lz77dt0.005bdump2000adump5000ens8\"\n",
    "bug_pairs = glob(macbookpro_path + '/eps*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.trans_foci_bug_cyl(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 min and 45 s for ~30000 particles with one all trj\n",
    "#macbookpro_path = \"/Users/amirhsi/Downloads/epss5epsl5r10.5al5nl5ml125ns400ac1nc27720lz77dt0.005bdump2000adump5000ens8\"\n",
    "macmini_path = '/Users/amirhsi_mini/research_data/test'\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob(macmini_path + '/eps*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob(macmini_path + '/eps*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = TransFociCyl(\n",
    "        all_trj,\n",
    "        topo_lineage,\n",
    "        'cylindrical',\n",
    "        group,\n",
    "        'ring'\n",
    "    )\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all_cyl(\n",
    "            all_topo, all_trj, lineage, save_to=save_to\n",
    "        )\n",
    "    else:\n",
    "        prober.trans_foci_all_cyl(\n",
    "            all_topo,\n",
    "            all_trj,\n",
    "            lineage,\n",
    "            save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing bug files.\\\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "group = 'bug'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "#macmini_path = \"/Users/amirhsi_mini/trjs/epss5.0epsl5.0r10.5al5.0nl5ml125ns200ac1.0nc*lz77.0dt0.005bdump5000adump5000ens1ring/*.bug*\"\n",
    "macbookpro_path = \"/Users/amirhsi_mini/research_data/\"\n",
    "bug_pairs = glob(macbookpro_path + '/al*' + group + '*')\n",
    "bug_pairs = organizer.sort_filenames(\n",
    "    bug_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "print(bug_pairs)\n",
    "for (bug_topo, bug_trj) in bug_pairs:\n",
    "    prober.trans_foci_bug_cub(\n",
    "        bug_topo,\n",
    "        bug_trj,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1 min and 45 s for ~30000 particles with one all trj\n",
    "macbookpro_path = \"/Users/amirhsi_mini/research_data/al1nl5ml1ns400ac1nc160392l36dt0.005bdump2000adump5000ens1.ring\"\n",
    "#macmini_path = '/Users/amirhsi_mini/research_data/test'\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "all_trjs = glob(macbookpro_path + '/al*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj.gz']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob(macbookpro_path + '/al*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = TransFociCub(all_trj, topo_lineage, 'cubic', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.trans_foci_all_cub(all_topo, all_trj, lineage, save_to=save_to)\n",
    "    else:\n",
    "        prober.trans_foci_all_cub(all_topo, all_trj, lineage, save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HNS project\n",
    "### Cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-21T16:25:07.676290Z",
     "iopub.status.busy": "2023-01-21T16:25:07.675154Z",
     "iopub.status.idle": "2023-01-21T16:25:07.687097Z",
     "shell.execute_reply": "2023-01-21T16:25:07.681899Z",
     "shell.execute_reply.started": "2023-01-21T16:25:07.676173Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3 min per nucleoid: \n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import HnsCub\n",
    "# analyzing bug files.\n",
    "group = 'nucleoid'\n",
    "lineage = 'whole'\n",
    "save_to = './'\n",
    "save_to = '/Users/amirhsi_mini/research_data/probes/' \n",
    "#probe_path = \"/Users/amirhsi_mini/research_data/hns_cubic-trjs/N*/N*\"\n",
    "#probe_path = \"../../Datasets/N200epshm29kbmm2nh48ac1nc95493l25dt0.005ndump2000adump5000ens1.ring\"\n",
    "#probe_path = \"../../Datasets/TestTrajectories/N200epshm29kbmm2nh48ac1nc95493l25dt0.005ndump2000adump5000ens1.ring-truncated_for_tests-No_backup-Keep_it\"\n",
    "probe_path = \"/Users/amirhsi_mini/research_data/trjs/N*\"\n",
    "nuc_pairs = glob(probe_path + '/N*' + group + '*')\n",
    "nuc_pairs = organizer.sort_filenames(\n",
    "    nuc_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "for (nuc_topo, nuc_trj) in nuc_pairs:\n",
    "    prober.hns_nucleoid_cub(\n",
    "        nuc_topo,\n",
    "        nuc_trj,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### segment all trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 4 min the largest.\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import HnsCub\n",
    "# ~6 min for 3 trjs, each with 100 frames\n",
    "probe_path = \"/Users/amirhsi_mini/research_data/trjs/N*\" \n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = './'\n",
    "save_to = '/Users/amirhsi_mini/research_data/probes/'\n",
    "all_topo = glob(probe_path + '/N*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "\n",
    "all_trjs = glob(probe_path + '/N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_trj in all_trjs:\n",
    "    trj_info = HnsCub(all_trj, topo_lineage, 'cubic', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.hns_all_cub(all_topo, all_trj, lineage, save_to=save_to)\n",
    "    else:\n",
    "        prober.hns_all_cub(all_topo, all_trj, lineage, save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cylindrical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### whole bug trjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import HnsCyl\n",
    "# analyzing bug files.\n",
    "group = 'nucleoid'\n",
    "lineage = 'whole'\n",
    "#save_to = './'\n",
    "save_to = '/Users/amirhsi_mini/research_data/HnsCyl-trjs/'\n",
    "probe_path = \"/Users/amirhsi_mini/research_data/HnsCyl-trjs/N*\"\n",
    "#probe_path = \"../../Datasets/N200kbmm2r4.5nh12ac2lz75nc552ens2.ring\"\n",
    "#probe_path = \"../../Datasets/TestTrajectories/N200epshm29kbmm2nh48ac1nc95493l25dt0.005ndump2000adump5000ens1.ring-truncated_for_tests-No_backup-Keep_it\"\n",
    "nuc_pairs = glob(probe_path + '/N*' + group + '*')\n",
    "nuc_pairs = organizer.sort_filenames(\n",
    "    nuc_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "print(nuc_pairs)\n",
    "for (nuc_topo, nuc_trj) in nuc_pairs:\n",
    "    prober.hns_nucleoid_cyl(\n",
    "        nuc_topo,\n",
    "        nuc_trj,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### hns_nucleoid_cyl_dis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import HnsCyl\n",
    "# analyzing bug files.\n",
    "group = 'nucleoid'\n",
    "lineage = 'whole'\n",
    "#save_to = './'\n",
    "save_to = '/Users/amirhsi_mini/research_data/HnsCyl-trjs/'\n",
    "probe_path = \"/Users/amirhsi_mini/research_data/HnsCyl-trjs/N*\"\n",
    "#probe_path = \"../../Datasets/N200kbmm2r4.5nh12ac2lz75nc552ens2.ring\"\n",
    "#probe_path = \"../../Datasets/TestTrajectories/N200epshm29kbmm2nh48ac1nc95493l25dt0.005ndump2000adump5000ens1.ring-truncated_for_tests-No_backup-Keep_it\"\n",
    "nuc_pairs = glob(probe_path + '/N*' + group + '*')\n",
    "nuc_pairs = organizer.sort_filenames(\n",
    "    nuc_pairs,\n",
    "    fmts=['.' + group + '.data', '.' + group + '.lammpstrj']\n",
    ")\n",
    "print(nuc_pairs)\n",
    "for (nuc_topo, nuc_trj) in nuc_pairs:\n",
    "    prober.hns_nucleoid_cyl_dis_matrix(\n",
    "        nuc_topo,\n",
    "        nuc_trj,\n",
    "        lineage,\n",
    "        save_to = save_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### segment all trj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import HnsCyl\n",
    "# ~6 min for 3 trjs, each with 100 frames\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = '/Users/amirhsi_mini/research_data/hns_cyl-trjs/'\n",
    "probe_path = \"/Users/amirhsi_mini/research_data/hns_cyl-trjs/N*\"\n",
    "all_trjs = glob(probe_path + '/N*' + group + '*')\n",
    "all_trjs = organizer.sort_filenames(\n",
    "    all_trjs,\n",
    "    fmts=['.' + group + '.lammpstrj']\n",
    ")\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob(probe_path + '/N*' + group + '*')\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "max_segment_id = len(all_trjs)\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "\n",
    "for all_trj in all_trjs:\n",
    "    print(all_trj)\n",
    "    trj_info = HnsCyl(all_trj, topo_lineage, 'cylindrical', group, 'ring')\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id == max_segment_id:\n",
    "        prober.hns_all_cyl(all_topo, all_trj, lineage, save_to=save_to)\n",
    "    else:\n",
    "        prober.hns_all_cyl(all_topo, all_trj, lineage, save_to=save_to,\n",
    "            continuous=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Loop on all topos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.probe import prober\n",
    "from polyphys.manage.parser import HnsCyl\n",
    "# ~6 min for 3 trjs, each with 100 frames\n",
    "group = 'all'\n",
    "topo_lineage = 'whole'\n",
    "lineage = 'segment'\n",
    "save_to = '/Users/amirhsi_mini/research_data/HnsCyl-trjs/'\n",
    "probe_path = \"/Users/amirhsi_mini/research_data/HnsCyl-trjs/N*\"\n",
    "all_topo = glob(probe_path + '/N*' + group + '*')\n",
    "all_topos = organizer.sort_filenames(all_topo, fmts=['.' + group + '.data'])\n",
    "all_topos = [all_topo[0] for all_topo in all_topos]\n",
    "# analyzig all files\n",
    "# it is assumed that the all trjs are numbers from 1 to max_segment_id\n",
    "for all_topo in all_topos:\n",
    "    print(all_topo)\n",
    "    dir_name = all_topo.split('/')[-1].split('.all')[0]\n",
    "    all_trjs = glob(probe_path[:-2] + '/'+ dir_name + '/N*' + group + '*')\n",
    "    all_trjs = organizer.sort_filenames(\n",
    "        all_trjs,\n",
    "        fmts=['.' + group + '.lammpstrj']\n",
    "    )\n",
    "    all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "    max_segment_id = len(all_trjs)\n",
    "    for all_trj in all_trjs:\n",
    "        trj_info = HnsCyl(all_trj, topo_lineage, 'cylindrical', group, 'ring')\n",
    "        # all the frames in the last segment are probed:\n",
    "        if trj_info.segment_id == max_segment_id:\n",
    "            prober.hns_all_cyl(all_topo, all_trj, lineage, save_to=save_to)\n",
    "        else:\n",
    "            prober.hns_all_cyl(all_topo, all_trj, lineage, save_to=save_to,\n",
    "                continuous=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDAnalysis as mda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "polylab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
