{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_CLXSgvVO_9"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.manage.parser import SumRule\n",
    "from polyphys.probe import prober\n",
    "from polyphys.analyze import analyzer\n",
    "from polyphys.analyze import distributions\n",
    "import MDAnalysis as mda\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# The probe phase:\n",
    "There are several ways of analyzing the topology and trajectory pairs, depending on the number of trajectory files per a topology file, the continuity of trjaectory files, organization of files in a directory, and the parallel or sequencial arrangement of the computation powerhorse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Separated *whole* simulation directories on a cluster: gnuparallel\n",
    "On the cluster, *whole* simulations are organized into *whole* directories, where each *whole* directory contains all the files for a given *whole* simulation. The **gnuparallel** is used to parallalize the **probe** phase at the **shell** level. For this purpose, all the python modules and scripts are separatedly installed and run on each core. For instance, if 32 cores are available, then the files in 32 *whole* directories are simulatenously installed. However, each *whole* directory may contains multiple toplogy and trajectory pairs. Thus, there is parallelization at the level of *whole* directories, not at the levle of the *segment* or *whole* trajectories inside a *whole* directory. Inside each *whole* directory, a python **main_probe.py** script analyzes the trajectories in a sequencial way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### trj anf all segments on a cluster\n",
    "For each *whole* directory, the following script is executed by means of *gnuparallel*. See these scripts: *probe-1.7-all_trj_segments.py* and *probe-1.7-bug_trj_segments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.manage.parser import SumRule\n",
    "from polyphys.probe import prober\n",
    "\n",
    "geometry = 'biaxial'\n",
    "trj_lineage = 'segment'\n",
    "save_to=\"./\"\n",
    "\n",
    "bug_trjs = glob(\"../test_data/trjs-continuous/N500D10.0ac0.8-trjs/N500epsilon5.0r5.5lz205.5sig0.8nc12012dt0.002bdump1000adump5000ens2/N*.bug.lammpstrj\")\n",
    "bug_trjs = organizer.sort_filenames(bug_trjs,fmts=['.bug.lammpstrj'])\n",
    "bug_trjs = [bug_trj[0] for bug_trj in bug_trjs]\n",
    "bug_topo = glob(\"../test_data/trjs-continuous/N500D10.0ac0.8-trjs/N500epsilon5.0r5.5lz205.5sig0.8nc12012dt0.002bdump1000adump5000ens2/N*.bug.data\")\n",
    "bug_topo = organizer.sort_filenames(bug_topo,fmts=['.bug.data'])\n",
    "bug_topo = bug_topo[0][0]\n",
    "print(bug_topo)\n",
    "for bug_trj in bug_trjs:\n",
    "    print(bug_trj)\n",
    "    trj_info = SumRule(bug_trj, geometry=geometry, group='bug',lineage=trj_lineage)\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id ==len(bug_trjs):\n",
    "        #print(\"last: \" + bug_trj)\n",
    "        prober.probe_bug(bug_topo, bug_trj, geometry, trj_lineage, save_to)\n",
    "    # the last frame in the all other segments is ignored:\n",
    "    else:\n",
    "        #print(bug_trj)\n",
    "        prober.probe_bug(bug_topo, bug_trj, geometry, trj_lineage, save_to, continuous=True)\n",
    "trj_lineage = 'segment'\n",
    "all_trjs = glob(\"../test_data/trjs-continuous/N500D10.0ac0.8-trjs/N500epsilon5.0r5.5lz205.5sig0.8nc12012dt0.002bdump1000adump5000ens2/N*.all.lammpstrj\")\n",
    "all_trjs = organizer.sort_filenames(all_trjs, fmts=['.all.lammpstrj'])\n",
    "all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "all_topo = glob(\"../test_data/trjs-continuous/N500D10.0ac0.8-trjs/N500epsilon5.0r5.5lz205.5sig0.8nc12012dt0.002bdump1000adump5000ens2/N*.all.data\")\n",
    "all_topo = organizer.sort_filenames(all_topo, fmts=['.all.data'])\n",
    "all_topo = all_topo[0][0]\n",
    "print(all_topo)\n",
    "for all_trj in all_trjs:\n",
    "    print(all_trj)\n",
    "    trj_info = SumRule(all_trj, geometry=geometry, group='all',lineage=trj_lineage)\n",
    "    # all the frames in the last segment are probed:\n",
    "    if trj_info.segment_id ==len(all_trjs):\n",
    "        #print(\"last: \" + all_trj)\n",
    "        prober.probe_all(all_topo, all_trj, geometry, trj_lineage, save_to)\n",
    "    # the last frame in the all other segments is ignored:\n",
    "    else:\n",
    "        #print(all_trj)\n",
    "        prober.probe_all(all_topo, all_trj, geometry, trj_lineage, save_to,continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Separated *whole* directories on a PC: Dask\n",
    "On a PC, the *whole* directories are located in a master *space-trjs* directory; however, one main python script probes all the *whole* directories in a parallel scheme via Dask. This is different from the *gnuparallel*-based approach in which each *whole* directory has its own copy of the required scripts and a main pytohn script is run to probe that direcotry individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This approach from HERE\n",
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "## to HERE, does not work of * is used in the string input for Path.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.bug.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.bug.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "for topology in topologies:\n",
    "    print(topology[0])\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            if trj_info.segment_id ==10:\n",
    "                prober.probe_bug(topology[0], trajectory[0], geometry, 'segment', save_to_whole)\n",
    "            else:\n",
    "                prober.probe_bug(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### bug whole trjs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### trjs whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This approach from HERE\n",
    "path = pathlib.Path('/Users/amirhsi_mini/trjs/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "## to HERE, does not work of * is used in the string input for Path.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.bug.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.bug.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "for topology in topologies:\n",
    "    print(topology[0])\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='whole')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            prober.probe_bug(topology[0], trajectory[0], geometry, 'whole', save_to_whole, continuous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### bug whole trjs dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This approach from HERE\n",
    "path = pathlib.Path('/Users/amirhsi_mini/trjs/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "## to HERE, does not work of * is used in the string input for Path.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.bug.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.bug.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "trjs_computed = []\n",
    "for topology in topologies:\n",
    "    print(topology[0])\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='whole')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            trj_delayed = delayed(prober.probe_bug)(topology[0], trajectory[0], geometry, 'whole', save_to_whole, continuous=False)\n",
    "            trjs_computed.append(trj_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# it takes 9min and 34s.\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Separated *segment* directories on a PC: Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.all.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.all.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "for topology in topologies:\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.segment_id ==10:\n",
    "            prober.probe_all(topology[0], trajectory[0], geometry, 'segment', save_to_whole)\n",
    "        else:\n",
    "            prober.probe_all(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "all_tuples =  organizer.sort_filenames(observations,fmts=['all.lammpstrj'])\n",
    "all_trjs = [all_tuple[0] for all_tuple in all_tuples]\n",
    "all_data =  organizer.sort_filenames(observations,fmts=['all.data'])\n",
    "all_data = all_data[0][0]\n",
    "\n",
    "    \n",
    "for all_trj in all_trjs:\n",
    "    print(all_trj)\n",
    "    #PipeLine.extract_trj_all(all_data, all_trj, geom, save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### trjs all segments dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('/Users/amirhsi_mini/trjs/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.all.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.all.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "trjs_computed = []\n",
    "for topology in topologies:\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            if trj_info.segment_id ==14:\n",
    "                trj_delayed = delayed(prober.probe_all_new)(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=False)\n",
    "                trjs_computed.append(trj_delayed)\n",
    "            else:\n",
    "                trj_delayed = delayed(prober.probe_all_new)(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)\n",
    "                trjs_computed.append(trj_delayed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# The analyze phase:\n",
    "In this phase, the segment files in the probe phase are merged into whole files. The ensemble, ensemble-averaged, and space files are created from whole files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes 25 min with nlags=100000\n",
    "input_database = '/Users/amirhsi_mini/probe/N2000D30.0ac4.0-segment/'\n",
    "#input_database = '../test_data/probe/N2000D30.0ac4.0-segment/'\n",
    "non_scalar_properties_bug = [\n",
    "    # property_, species, group\n",
    "    ('principalT', 'Mon', 'bug'),\n",
    "]\n",
    "\n",
    "acf_tseries_properties_bug = [\n",
    "    # property_, species, group\n",
    "    ('fsdT', 'Mon', 'bug'),\n",
    "    ('gyrT', 'Mon', 'bug'),\n",
    "    ('rfloryT', 'Mon', 'bug'),\n",
    "    ('shapeT', 'Mon', 'bug'),\n",
    "    ('asphericityT', 'Mon', 'bug')\n",
    "]\n",
    "\n",
    "hist_properties_bug = [\n",
    "    # direction, species, group\n",
    "    ('rflory', 'Mon', 'bug')\n",
    "]\n",
    "geometry = 'biaxial'\n",
    "analyzer.analyze_segments_bug(\n",
    "    input_database,\n",
    "    #non_scalar_properties=non_scalar_properties_bug,\n",
    "    acf_tseries_properties=acf_tseries_properties_bug,\n",
    "    #hist_properties=hist_properties_bug,\n",
    "    geometry=geometry,\n",
    "    hierarchy='N*/N*',\n",
    "    nlags=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllInOne Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming convention:\n",
    "This is the pattern of file or directory names:\n",
    "\n",
    "1. **whole** files: whole-group-property_[-measure][-stage][.ext]\n",
    "2. **ensemble** files: ensemble-group-property_[-measure][-stage][.ext]\n",
    "3. **ensemble_long** files: ensemble_long-group-property_[-measure][-stage][.ext]\n",
    "4. **space** files: space-group-property_[-measure][-stage][.ext]\n",
    "5. **all in one** files: **allInOne**-group-property_[-measure][-stage][.ext]\n",
    "\n",
    "[keyword] means that the keyword in the file name is option. [-measure] is a physical measurement such as the auto correlation function (AFC) done on the physical 'property_'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique property_measures:\n",
    "database = '/Users/amirhsi_mini/analysis/'\n",
    "bug_property_measures = glob(database+\"/N*-ensAvg\"+\"/N*.csv\")\n",
    "bug_property_measures = list(set([\"-\".join(property_measure.split(\"/\")[-1].split(\".csv\")[0] .split(\"-\")[2:]) for property_measure in bug_property_measures]))\n",
    "bug_property_measures.remove(\"stamps-ensAvg\")\n",
    "bug_property_measures.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain-size timeseries and their associated measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating property_measures of kinds timeseries and timesseries acfs:\n",
    "bug_property_acfs = list()\n",
    "for property_measure in bug_property_measures:\n",
    "    if \"-acf\" in property_measure:\n",
    "        bug_property_acfs.append(property_measure)\n",
    "bug_property_acfs.sort()\n",
    "print(bug_property_acfs)\n",
    "# chain timeseries:\n",
    "bug_properties = list()\n",
    "for property_measure in bug_property_acfs:\n",
    "    if \"-acf-\" in property_measure:\n",
    "        bug_properties.append(property_measure.split(\"-\")[0]+'-ensAvg')\n",
    "bug_properties.sort()\n",
    "print(bug_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allInOne timeseries for chain-size statistics\n",
    "group = 'bug'\n",
    "geometry = 'biaxial'\n",
    "ensAvg_path = \"/Users/amirhsi_mini/analysis/N2000D30.0ac4.0-bug-ensAvg\"\n",
    "ensAvgs = []\n",
    "for property_measure in bug_properties:\n",
    "    ensAvg = organizer.all_in_one_tseries(\n",
    "        ensAvg_path,\n",
    "        property_measure,\n",
    "        group = group,\n",
    "        geometry = geometry,\n",
    "        save_to = None\n",
    "    )\n",
    "    ensAvgs.append(ensAvg)\n",
    "ensAvgs = pd.concat(ensAvgs,axis=1)\n",
    "# drop duplicated columns:\n",
    "ensAvgs = ensAvgs.loc[:,~ensAvgs.columns.duplicated()]\n",
    "output_name = database + \"allInOne-bug-chainSize.csv\"\n",
    "ensAvgs.to_csv(output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all in one timeseries for chain-szie acf statistics\n",
    "group = 'bug'\n",
    "geometry = 'biaxial'\n",
    "ensAvg_path = \"/Users/amirhsi_mini/analysis/N2000D30.0ac4.0-bug-ensAvg\"\n",
    "ensAvgs = list()\n",
    "for property_measure in bug_property_acfs:\n",
    "    ensAvg = organizer.all_in_one_tseries(\n",
    "        ensAvg_path,\n",
    "        property_measure,\n",
    "        group = group,\n",
    "        geometry = geometry,\n",
    "        save_to = None\n",
    "    )\n",
    "    ensAvgs.append(ensAvg)\n",
    "ensAvgs = pd.concat(ensAvgs,axis=1)\n",
    "# drop duplicated columns:\n",
    "ensAvgs = ensAvgs.loc[:,~ensAvgs.columns.duplicated()]\n",
    "output_name = database + \"allInOne-bug-chainSize-acf.csv\"\n",
    "ensAvgs.to_csv(output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parallel version has memory leak issue.\n",
    "%%time\n",
    "# This has memory leaking issue\n",
    "group = 'bug'\n",
    "geometry = 'biaxial'\n",
    "ensAvg_path = \"/Users/amirhsi_mini/analysis/N2000D30.0ac4.0-bug-ensAvg\"\n",
    "all_in_one_computed = []\n",
    "for property_measure in bug_property_measures:\n",
    "    all_in_one_delayed = delayed(organizer.all_in_one_tseries)(\n",
    "        ensAvg_path,\n",
    "        property_measure,\n",
    "        group = group,\n",
    "        geometry = geometry,\n",
    "        save_to = database\n",
    "    )\n",
    "    all_in_one_computed.append(all_in_one_delayed)\n",
    "_ = compute(all_in_one_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_paths = glob('/Users/amirhsi_mini/probe/N500D10.0ac0.8-segment/N500epsilon5.0r5.5lz205.5sig0.8nc12012dt0.002bdump1000adump5000ens1/N500epsilon5.0r5.5lz205.5sig0.8nc12012dt0.002bdump1000adump5000ens1*')\n",
    "hist_paths = glob('/Users/amirhsi_mini/probe/N500D10.0ac0.8-segment/N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1/N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1*')\n",
    "species = 'Crd'\n",
    "direction = 'z'\n",
    "geometry='biaxial'\n",
    "group='all'\n",
    "segments = organizer.sort_filenames(\n",
    "                hist_paths,\n",
    "                fmts=['-' + direction + 'Hist' + species + '.npy']\n",
    "            )\n",
    "edge_segments = organizer.sort_filenames(\n",
    "                hist_paths,\n",
    "                fmts=['-' + direction + 'Edge' + species + '.npy']\n",
    "            )\n",
    "wholes = organizer.whole(\n",
    "                direction + 'Hist' + species,\n",
    "                segments,\n",
    "                geometry=geometry,\n",
    "                group=group,\n",
    "                relation='histogram',\n",
    "                save_to=None\n",
    "            )\n",
    "edge_wholes = organizer.whole(\n",
    "                direction + 'Edge' + species,\n",
    "                edge_segments,\n",
    "                geometry=geometry,\n",
    "                group=group,\n",
    "                relation='bin_edge',\n",
    "                save_to=None\n",
    "            )\n",
    "            # 'whole' dataframes, each with a 'whole' columns.\n",
    "rho_wholes, phi_wholes = distributions.distributions_generator(\n",
    "                wholes,\n",
    "                edge_wholes,\n",
    "                group,\n",
    "                species,\n",
    "                geometry,\n",
    "                direction,\n",
    "                save_to=None,\n",
    "normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_wholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'][:-1],edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'],weights=wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'],histtype='step',density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'][:-1],bins=edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'],weights=wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1,ncols=1,sharex=True,figsize=(8,6))\n",
    "centers = 0.5*(edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'][:-1]+edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'][1:])\n",
    "hist_df = pd.DataFrame(wholes)\n",
    "rho_df = pd.DataFrame(rho_wholes)\n",
    "phi_df = pd.DataFrame(phi_wholes)\n",
    "df = pd.concat([hist_df,rho_df,phi_df],axis=1)\n",
    "df.columns = ['histogram','number_density','volume_fraction']\n",
    "df['center'] = centers\n",
    "#df['histogram'] = df['histogram'] / df['histogram'].sum()\n",
    "df['fake']= 1\n",
    "#df.set_index('center',inplace=True)\n",
    "#sns.histplot(x='center',bins=edge_wholes['N500epsilon5.0r5.5lz205.5sig0.8nc48047dt0.002bdump1000adump5000ens1'] ,weights='volume_fraction',data=df,element='poly',fill=False, kde=True)\n",
    "#plt.show()\n",
    "#df['histogram'].plot(ax=axes,ylabel='histogram')\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "#sns.set(font_scale=1.2)\n",
    "sns.axes_style(\"darkgrid\")\n",
    "sns.lineplot(x='center',y='histogram', data=df,ax=axes)\n",
    "#df.loc[-200:200,'number_density'].plot(ax=axes[1],ylabel='number_density')\n",
    "#df.loc[-200:200,'volume_fraction'].plot(ax=axes[2],ylabel='volume_fraction',xlabel='center')\n",
    "#axes.grid()\n",
    "#axes.set_xlim(df.index[0]-5, df.index[-1]+5)\n",
    "#axes.axvline(df.loc[df.index[0],'center'],lw=0.5,c='red',label='left end')\n",
    "#axes.axvline(df.loc[df.index[-1],'center'],lw=0.5,c='green',label='right end')\n",
    "#axes.axvline(df['center'],lw=0.5,c='red')\n",
    "axes.set_xlabel('z (a.u.)')\n",
    "axes.set_ylabel('Freqency of type-1 particles')\n",
    "#ax.set_xlim[]\n",
    "plt.savefig('histogram.pdf',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['center'][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'N500epsilon5.0r5.5lz205.5sig0.8nc36036dt0.002bdump1000adump5000ens1'\n",
    "hist_info = SumRule(name, geometry='biaxial', group='all', lineage='whole')\n",
    "dist_new = distributions.Distribution(\n",
    "    wholes[name],\n",
    "    edges[name],\n",
    "    hist_info,\n",
    "    'dcrowd',\n",
    "    geometry='biaxial',\n",
    "    direction='z',\n",
    "    normalized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whole_from_Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_database = '/Users/amirhsi_mini/probe/N500D10.0ac0.8-segment'\n",
    "geometry = 'biaxial'\n",
    "hierarchy = '/N*/N*'\n",
    "lineage = 'segment'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "#save_to = analyzer.database_path(input_database, phase='analysis', stage='wholeSim', group=group)\n",
    "#analyzer.analyze_segments(input_database, geometry, hierarchy)\n",
    "analyzer.analyze_wholes(input_database, geometry, hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script is used in GNU-Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PipeLine import *\n",
    "\n",
    "fname = glob(\"../N*.bug.*\")\n",
    "fname = PipeLine.file_reader(fname) # This is a list with one member\n",
    "\n",
    "save_to=\"./\"\n",
    "geom = 'cylindrical'\n",
    "print(fname)\n",
    "PipeLine.extract_trj_bug(fname[0], geom, save_to) # A list with one member, the member is a tuple of a trj and data pair.\n",
    "#PipeLine.bug_trj_rmsd(fname[0], geom, save_to) \n",
    "\n",
    "trj_files = glob(\"./N*all.lammpstrj\")\n",
    "all_tuples = PipeLine.file_reader(trj_files,extensions=['lammpstrj'])\n",
    "all_trjs = [all_tuple[0] for all_tuple in all_tuples]\n",
    "\n",
    "data_file = glob(\"./N*.all.data\")\n",
    "all_data = PipeLine.file_reader(data_file,extensions=['all.data'])\n",
    "all_data = all_data[0][0]\n",
    "\n",
    "    \n",
    "for all_trj in all_trjs:\n",
    "    print(all_trj)\n",
    "    PipeLine.extract_trj_all(all_data, all_trj, geom, save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard approach: Running on clusters: extraction from orgaznied *trjs_all* and *trjs_bug* directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This are not work properly on Graham cluster but work well on iMacmini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract from an organized *trjs_bug* directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extract different bug's information from pairs (toplogy and trajectory) of bug simulation files in oen or more organized *trjs_bug* directories.\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "from PipeLine import *\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "\n",
    "cores = 32\n",
    "print(f\"number of workers set to {cores}; is this the same requested cores on the cluster?\")\n",
    "client = Client(n_workers=cores)\n",
    "home = str(Path.home())\n",
    "cwdir = str(Path.cwd())\n",
    "# information extraction from simulations\n",
    "geom = 'cylindrical'\n",
    "fname = glob(home+'/amirhsi_rrg/cylinder_simulations/N*-trjs_bug/N*bug*')\n",
    "bug_pairs = PipeLine.file_reader(fname) # each bug_pair is a pair of trajectory and topopgy file.\n",
    "trjs_computed = []\n",
    "bug_dir = 'extraction_bug/'\n",
    "for bug_pair in bug_pairs:\n",
    "    sim_name = bug_pair[0].split(\"/\")[-1].split('bug')[0]\n",
    "    sim_dir = cwdir+bug_dir+sim_name\n",
    "    Path(sim_dir).mkdir(parents=True, exist_ok=False)\n",
    "    sim_save_to = sim_dir+\"/\"\n",
    "    trj_delayed = delayed(PipeLine.extract_trj_bug)(bug_pair, geom,sim_save_to)\n",
    "    trjs_computed.append(trj_delayed)\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract from an organized *trjs_all* directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extract different bug's information from pairs (toplogy and trajectory) of bug simulation files in oen or more organized *trjs_bug* directories.\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "from PipeLine import *\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "\n",
    "cores = 32\n",
    "print(f\"number of workers set to {cores}; is this the same requested cores on the cluster?\")\n",
    "client = Client(n_workers=cores)\n",
    "home = str(Path.home())\n",
    "cwdir = str(Path.cwd())\n",
    "sim_all_dirs = glob(home+'/amirhsi_rrg/cylinder_simulations/N*-trjs_all/N*/')\n",
    "geom = 'cylindrical'\n",
    "\n",
    "trjs_computed = []\n",
    "all_extraction_dir = 'extraction_all/'\n",
    "for sim_all_dir in sim_all_dirs:\n",
    "    sim_name = sim_all_dir[0].split(\"/\")[-1]\n",
    "    all_trjs = glob(sim_all_dir+\"N*.lammpstrj\")\n",
    "    all_trjs = PipeLine.file_reader(all_trjs,extensions=['lammpstrj'])\n",
    "    all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "\n",
    "    all_topology = glob(sim_all_dir+\"N*.all.data\")\n",
    "    all_topology = PipeLine.file_reader(all_topology,extensions=['all.data'])\n",
    "    all_topology = all_topology[0][0]\n",
    "    \n",
    "    \n",
    "    sim_extract_dir = cwdir+all_extraction_dir+sim_name\n",
    "    Path(sim_extract_dir).mkdir(parents=True, exist_ok=False)\n",
    "    sim_save_to = sim_extract_dir+\"/\"\n",
    "    \n",
    "    for all_trj in all_trjs:\n",
    "        trj_delayed = delayed(PipeLine.extract_trj_all)(all_topology, all_trj, geom,sim_save_To)\n",
    "        trjs_computed.append(trj_delayed)\n",
    "\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction from *extraction_bug* directory after a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = str(Path.home())\n",
    "path=home+'N2000epsilon5.0r10.5lz336sig1.0nc100800dt0.005bdump1000adump5000ens1'\n",
    "fname = glob(path+\"/N*.bug.*\")\n",
    "fname = PipeLine.file_reader(fname) # This is a list with one member\n",
    "geom = 'cylindrical'\n",
    "print(fname)\n",
    "PipeLine.extract_trj_bug(fname[0], geom) # A list with one member, the member is a tuple of a trj and data pair.\n",
    "PipeLine.rmsd_trj_bug(fname[0], geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the segments in one trajectory: M dump files + one data file.\n",
    "path='/Users/amirhsi_mini/N2000epsilon5.0r10.5lz336sig1.0nc100800dt0.005bdump1000adump5000ens*'\n",
    "geom = 'cylindrical'\n",
    "trj_files = glob(path+\"/N*.lammpstrj\")\n",
    "all_tuples = PipeLine.file_reader(trj_files,extensions=['lammpstrj',])\n",
    "all_trjs = [all_tuple[0] for all_tuple in all_tuples]\n",
    "data_file = glob(path+\"/N*.all.data\")\n",
    "all_data = PipeLine.file_reader(data_file,extensions=['all.data'])\n",
    "all_data = all_data[0][0]\n",
    "for all_trj in all_trjs:\n",
    "    PipeLine.extract_trj_all(all_data, all_trj, geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach: tested on iMac Pro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single ensemble with one or more segments with one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/amirhsi_mini/N2000epsilon5.0r15.5lz379.5sig6.0nc1068dt0.005bdump1000adump5000ens*'\n",
    "#path='/Users/amirhsi_mini/N1000epsilon5.0r8.0lz308.5sig2.0nc10412dt0.005bdump1000adump5000ens*'\n",
    "geom = 'cylindrical'\n",
    "trj_files = glob(path+\"/N*all*\")\n",
    "all_pairs = PipeLine.file_reader(trj_files)\n",
    "trjs_computed = []\n",
    "for all_pair in all_pairs:\n",
    "    trj_delayed = delayed(PipeLine.extract_all_trj_polymer_cog_fsd)(all_pair[1], all_pair[0], geom)\n",
    "    trjs_computed.append(trj_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N ensemble with N data file, each ensemble with one or more segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/amirhsi_mini/N2000epsilon5.0r15.5lz379.5sig6.0nc1068dt0.005bdump1000adump5000ens*'\n",
    "#path='/Users/amirhsi_mini/N1000epsilon5.0r8.0lz308.5sig2.0nc10412dt0.005bdump1000adump5000ens*'\n",
    "geom = 'cylindrical'\n",
    "trj_pathes = glob(path+\"/N*all*\")\n",
    "trjs = PipeLine.file_reader(trj_pathes,extensions=['lammpstrj'])\n",
    "trjs = [trj[0] for trj in trjs]\n",
    "topology_pathes = glob(path+\"/N*.all.data\")\n",
    "topologies = PipeLine.file_reader(topology_pathes,extensions=['all.data'])\n",
    "topologies = [topology[0] for topology in topologies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(pair):\n",
    "    \"\"\"\n",
    "    simulation_pair pairs an \"all\" topology file with all the \"all\" trjectories of that \"all\" topology.\n",
    "    \n",
    "    Parameters:\n",
    "    pair (list of tuples): a list in whic each tuple is  pair of topolgy and trajectories of a simulations.\n",
    "    \n",
    "    Return:\n",
    "    a dict of of simulation pairs.\n",
    "    \"\"\"\n",
    "    return {'topology':pair[0], 'trajectories':pair[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_names = [topology.split(\"/\")[-1].split('.all')[0] for topology in topologies]\n",
    "ens_names = list(dict.fromkeys(ens_names))\n",
    "trjs_per_ens = []\n",
    "for ens_name in ens_names:\n",
    "    ens_trjs = []\n",
    "    for trj in trjs:\n",
    "        trj_name = trj.split(\"/\")[-1].split(\".all\")[0]\n",
    "        if trj_name == ens_name:\n",
    "            ens_trjs.append(trj)\n",
    "            #ensembles[key]['trajectories'] = trj\n",
    "    trjs_per_ens.append(ens_trjs)\n",
    "ensembles= dict(zip(ens_names,list(map(simulation,list(zip(topologies,trjs_per_ens))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = 'cylindrical'\n",
    "trjs_computed = []\n",
    "for ensemble in ensembles.values():\n",
    "    for trj_segment in ensemble['trajectories']:\n",
    "        trj_delayed = delayed(PipeLine.extract_trj_all)(ensemble['topology'], trj_segment, geom)\n",
    "        trjs_computed.append(trj_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = compute(trjs_computed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
