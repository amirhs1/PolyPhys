{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_CLXSgvVO_9"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "from typing import Dict, Tuple, Type, Optional\n",
    "import scipy.integrate as integrate\n",
    "from polyphys.manage import organizer\n",
    "from polyphys.manage.parser import SumRule\n",
    "from polyphys.probe import prober\n",
    "from polyphys.analyze import analyzer\n",
    "from polyphys.analyze import distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## test probe bug trj segments on pc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This approach from HERE\n",
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "## to HERE, does not work of * is used in the string input for Path.\n",
    "geometry = 'biaxial'\n",
    "group = 'bug'\n",
    "hierarchy = '/N*/N*'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.bug.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.bug.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "for topology in topologies:\n",
    "    print(topology[0])\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.whole == topo_info.whole:\n",
    "            if trj_info.segment_id ==10:\n",
    "                prober.probe_bug(topology[0], trajectory[0], geometry, 'segment', save_to_whole)\n",
    "            else:\n",
    "                prober.probe_bug(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check probe all trj segments on pc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path('../test_data/trjs-continuous/N500D10.0ac0.8-trjs')\n",
    "path = path.resolve() # convert relative path to aabsolute one\n",
    "input_database = str(path)\n",
    "geometry = 'biaxial'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "if not pathlib.Path(input_database).exists():\n",
    "    raise OSError(f\"'{input_database}'\"\n",
    "                    \"path does not exist.\")\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "topologies = organizer.sort_filenames(observations, fmts=['.all.data'])\n",
    "trajectories = organizer.sort_filenames(observations, fmts=['.all.lammpstrj'])\n",
    "# 'bug' time series and historams\n",
    "save_to = analyzer.database_path(input_database, phase='probe', stage='segment', group=None)\n",
    "for topology in topologies:\n",
    "    topo_info = SumRule(topology[0],geometry=geometry, group=group, lineage='whole')\n",
    "    save_to_whole = save_to + '/' + topo_info.whole\n",
    "    save_to_whole = pathlib.Path(save_to_whole) \n",
    "    try:\n",
    "        save_to_whole.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError as error:\n",
    "        print(error)\n",
    "        print(\n",
    "            f\"Directory '{save_to_whole}'\"\n",
    "            \" exist. Files are saved/overwritten to an existing directoy.\")\n",
    "    finally:\n",
    "        save_to_whole = str(save_to_whole) + '/'\n",
    "    for trajectory in trajectories:\n",
    "        trj_info = SumRule(trajectory[0],geometry=geometry, group=group, lineage='segment')\n",
    "        if trj_info.segment_id ==10:\n",
    "            prober.probe_all(topology[0], trajectory[0], geometry, 'segment', save_to_whole)\n",
    "        else:\n",
    "            prober.probe_all(topology[0], trajectory[0], geometry, 'segment', save_to_whole, continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "geometry = 'biaxial'\n",
    "phase = 'analysis'\n",
    "stage = 'wholeSim'\n",
    "group = 'bug'\n",
    "hierarchy = '/N*/N*'\n",
    "relation = 'tseries'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "observations = organizer.sort_filenames(observations, fmts=['bug-gyrTMon.npy'])\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "gyr_wholes = organizer.whole('gyrTMon', observations, geometry=geometry, group=group, relation=relation, save_to=save_to)\n",
    "\n",
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "geometry = 'biaxial'\n",
    "phase = 'analysis'\n",
    "stage = 'wholeSim'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "relation = 'histogram'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "observations = organizer.sort_filenames(observations, fmts=['-all-rHistMon.npy'])\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "rhist_wholes = organizer.whole('rHistMon', observations, geometry=geometry, group=group, relation=relation, save_to=save_to)\n",
    "\n",
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "geometry = 'biaxial'\n",
    "phase = 'analysis'\n",
    "stage = 'wholeSim'\n",
    "group = 'all'\n",
    "hierarchy = '/N*/N*'\n",
    "relation = 'bin_edge'\n",
    "observations = glob(input_database + hierarchy)\n",
    "if observations == []:\n",
    "    raise OSError(\n",
    "        \"File not found in \"\n",
    "        f\"'{input_database + hierarchy}'\"\n",
    "        )\n",
    "observations = organizer.sort_filenames(observations, fmts=['-all-rEdgeMon.npy'])\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "redge_wholes = organizer.whole('rEdgeMon', observations, geometry=geometry, group=group, relation=relation, save_to=save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'bug'\n",
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "phase = 'analysis'\n",
    "stage = 'ens'\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "gyr_ens = organizer.ensemble(\n",
    "    'gyrTMon',\n",
    "    gyr_wholes,\n",
    "    group=group,\n",
    "    edge_wholes=None,\n",
    "    save_to=save_to)\n",
    "group = 'all'\n",
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "phase = 'analysis'\n",
    "stage = 'ens'\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "rhist_ens = organizer.ensemble(\n",
    "    'rHistMon',\n",
    "    rhist_wholes,\n",
    "    group=group,\n",
    "    edge_wholes=redge_wholes,\n",
    "    save_to=save_to)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group = 'bug'\n",
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "phase = 'analysis'\n",
    "stage = 'ensAvg'\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "gyr_ens_avg = analyzer.ensemble_avg(\n",
    "    'gyrTMon',\n",
    "    gyr_ens,\n",
    "    group=group,\n",
    "    save_to=save_to)\n",
    "group = 'all'\n",
    "input_database = '../test_data/probe/N500D10.0ac0.8-segment'\n",
    "phase = 'analysis'\n",
    "stage = 'ensAvg'\n",
    "save_to = analyzer.database_path(input_database, phase=phase, stage=stage, group=group)\n",
    "rhist_ens_avg = analyzer.ensemble_avg(\n",
    "    'rHistMon',\n",
    "    rhist_ens,\n",
    "    group=group,\n",
    "    exclude=['bin_center'],\n",
    "    save_to=save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group = 'all'\n",
    "species = 'Mon'\n",
    "geometry = 'biaxial'\n",
    "direction = 'r'\n",
    "rho, phi = distributions.distributions_generator(\n",
    "    rhist_wholes,\n",
    "    redge_wholes,\n",
    "    group,\n",
    "    species,\n",
    "    geometry,\n",
    "    direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PipeLine import *\n",
    "\n",
    "fname = glob(\"../N*.bug.*\")\n",
    "fname = PipeLine.file_reader(fname) # This is a list with one member\n",
    "\n",
    "save_to=\"./\"\n",
    "geom = 'cylindrical'\n",
    "print(fname)\n",
    "PipeLine.extract_trj_bug(fname[0], geom, save_to) # A list with one member, the member is a tuple of a trj and data pair.\n",
    "#PipeLine.bug_trj_rmsd(fname[0], geom, save_to) \n",
    "\n",
    "trj_files = glob(\"./N*all.lammpstrj\")\n",
    "all_tuples = PipeLine.file_reader(trj_files,extensions=['lammpstrj'])\n",
    "all_trjs = [all_tuple[0] for all_tuple in all_tuples]\n",
    "\n",
    "data_file = glob(\"./N*.all.data\")\n",
    "all_data = PipeLine.file_reader(data_file,extensions=['all.data'])\n",
    "all_data = all_data[0][0]\n",
    "\n",
    "    \n",
    "for all_trj in all_trjs:\n",
    "    print(all_trj)\n",
    "    PipeLine.extract_trj_all(all_data, all_trj, geom, save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard approach: Running on clusters: extraction from orgaznied *trjs_all* and *trjs_bug* directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This are not work properly on Graham cluster but work well on iMacmini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract from an organized *trjs_bug* directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extract different bug's information from pairs (toplogy and trajectory) of bug simulation files in oen or more organized *trjs_bug* directories.\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "from PipeLine import *\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "\n",
    "cores = 32\n",
    "print(f\"number of workers set to {cores}; is this the same requested cores on the cluster?\")\n",
    "client = Client(n_workers=cores)\n",
    "home = str(Path.home())\n",
    "cwdir = str(Path.cwd())\n",
    "# information extraction from simulations\n",
    "geom = 'cylindrical'\n",
    "fname = glob(home+'/amirhsi_rrg/cylinder_simulations/N*-trjs_bug/N*bug*')\n",
    "bug_pairs = PipeLine.file_reader(fname) # each bug_pair is a pair of trajectory and topopgy file.\n",
    "trjs_computed = []\n",
    "bug_dir = 'extraction_bug/'\n",
    "for bug_pair in bug_pairs:\n",
    "    sim_name = bug_pair[0].split(\"/\")[-1].split('bug')[0]\n",
    "    sim_dir = cwdir+bug_dir+sim_name\n",
    "    Path(sim_dir).mkdir(parents=True, exist_ok=False)\n",
    "    sim_save_to = sim_dir+\"/\"\n",
    "    trj_delayed = delayed(PipeLine.extract_trj_bug)(bug_pair, geom,sim_save_to)\n",
    "    trjs_computed.append(trj_delayed)\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract from an organized *trjs_all* directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extract different bug's information from pairs (toplogy and trajectory) of bug simulation files in oen or more organized *trjs_bug* directories.\n",
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "from PipeLine import *\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "\n",
    "cores = 32\n",
    "print(f\"number of workers set to {cores}; is this the same requested cores on the cluster?\")\n",
    "client = Client(n_workers=cores)\n",
    "home = str(Path.home())\n",
    "cwdir = str(Path.cwd())\n",
    "sim_all_dirs = glob(home+'/amirhsi_rrg/cylinder_simulations/N*-trjs_all/N*/')\n",
    "geom = 'cylindrical'\n",
    "\n",
    "trjs_computed = []\n",
    "all_extraction_dir = 'extraction_all/'\n",
    "for sim_all_dir in sim_all_dirs:\n",
    "    sim_name = sim_all_dir[0].split(\"/\")[-1]\n",
    "    all_trjs = glob(sim_all_dir+\"N*.lammpstrj\")\n",
    "    all_trjs = PipeLine.file_reader(all_trjs,extensions=['lammpstrj'])\n",
    "    all_trjs = [all_trj[0] for all_trj in all_trjs]\n",
    "\n",
    "    all_topology = glob(sim_all_dir+\"N*.all.data\")\n",
    "    all_topology = PipeLine.file_reader(all_topology,extensions=['all.data'])\n",
    "    all_topology = all_topology[0][0]\n",
    "    \n",
    "    \n",
    "    sim_extract_dir = cwdir+all_extraction_dir+sim_name\n",
    "    Path(sim_extract_dir).mkdir(parents=True, exist_ok=False)\n",
    "    sim_save_to = sim_extract_dir+\"/\"\n",
    "    \n",
    "    for all_trj in all_trjs:\n",
    "        trj_delayed = delayed(PipeLine.extract_trj_all)(all_topology, all_trj, geom,sim_save_To)\n",
    "        trjs_computed.append(trj_delayed)\n",
    "\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction from *extraction_bug* directory after a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = str(Path.home())\n",
    "path=home+'N2000epsilon5.0r10.5lz336sig1.0nc100800dt0.005bdump1000adump5000ens1'\n",
    "fname = glob(path+\"/N*.bug.*\")\n",
    "fname = PipeLine.file_reader(fname) # This is a list with one member\n",
    "geom = 'cylindrical'\n",
    "print(fname)\n",
    "PipeLine.extract_trj_bug(fname[0], geom) # A list with one member, the member is a tuple of a trj and data pair.\n",
    "PipeLine.rmsd_trj_bug(fname[0], geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the segments in one trajectory: M dump files + one data file.\n",
    "path='/Users/amirhsi_mini/N2000epsilon5.0r10.5lz336sig1.0nc100800dt0.005bdump1000adump5000ens*'\n",
    "geom = 'cylindrical'\n",
    "trj_files = glob(path+\"/N*.lammpstrj\")\n",
    "all_tuples = PipeLine.file_reader(trj_files,extensions=['lammpstrj',])\n",
    "all_trjs = [all_tuple[0] for all_tuple in all_tuples]\n",
    "data_file = glob(path+\"/N*.all.data\")\n",
    "all_data = PipeLine.file_reader(data_file,extensions=['all.data'])\n",
    "all_data = all_data[0][0]\n",
    "for all_trj in all_trjs:\n",
    "    PipeLine.extract_trj_all(all_data, all_trj, geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach: tested on iMac Pro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single ensemble with one or more segments with one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/amirhsi_mini/N2000epsilon5.0r15.5lz379.5sig6.0nc1068dt0.005bdump1000adump5000ens*'\n",
    "#path='/Users/amirhsi_mini/N1000epsilon5.0r8.0lz308.5sig2.0nc10412dt0.005bdump1000adump5000ens*'\n",
    "geom = 'cylindrical'\n",
    "trj_files = glob(path+\"/N*all*\")\n",
    "all_pairs = PipeLine.file_reader(trj_files)\n",
    "trjs_computed = []\n",
    "for all_pair in all_pairs:\n",
    "    trj_delayed = delayed(PipeLine.extract_all_trj_polymer_cog_fsd)(all_pair[1], all_pair[0], geom)\n",
    "    trjs_computed.append(trj_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = compute(trjs_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N ensemble with N data file, each ensemble with one or more segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/amirhsi_mini/N2000epsilon5.0r15.5lz379.5sig6.0nc1068dt0.005bdump1000adump5000ens*'\n",
    "#path='/Users/amirhsi_mini/N1000epsilon5.0r8.0lz308.5sig2.0nc10412dt0.005bdump1000adump5000ens*'\n",
    "geom = 'cylindrical'\n",
    "trj_pathes = glob(path+\"/N*all*\")\n",
    "trjs = PipeLine.file_reader(trj_pathes,extensions=['lammpstrj'])\n",
    "trjs = [trj[0] for trj in trjs]\n",
    "topology_pathes = glob(path+\"/N*.all.data\")\n",
    "topologies = PipeLine.file_reader(topology_pathes,extensions=['all.data'])\n",
    "topologies = [topology[0] for topology in topologies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(pair):\n",
    "    \"\"\"\n",
    "    simulation_pair pairs an \"all\" topology file with all the \"all\" trjectories of that \"all\" topology.\n",
    "    \n",
    "    Parameters:\n",
    "    pair (list of tuples): a list in whic each tuple is  pair of topolgy and trajectories of a simulations.\n",
    "    \n",
    "    Return:\n",
    "    a dict of of simulation pairs.\n",
    "    \"\"\"\n",
    "    return {'topology':pair[0], 'trajectories':pair[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_names = [topology.split(\"/\")[-1].split('.all')[0] for topology in topologies]\n",
    "ens_names = list(dict.fromkeys(ens_names))\n",
    "trjs_per_ens = []\n",
    "for ens_name in ens_names:\n",
    "    ens_trjs = []\n",
    "    for trj in trjs:\n",
    "        trj_name = trj.split(\"/\")[-1].split(\".all\")[0]\n",
    "        if trj_name == ens_name:\n",
    "            ens_trjs.append(trj)\n",
    "            #ensembles[key]['trajectories'] = trj\n",
    "    trjs_per_ens.append(ens_trjs)\n",
    "ensembles= dict(zip(ens_names,list(map(simulation,list(zip(topologies,trjs_per_ens))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = 'cylindrical'\n",
    "trjs_computed = []\n",
    "for ensemble in ensembles.values():\n",
    "    for trj_segment in ensemble['trajectories']:\n",
    "        trj_delayed = delayed(PipeLine.extract_trj_all)(ensemble['topology'], trj_segment, geom)\n",
    "        trjs_computed.append(trj_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = compute(trjs_computed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
